import pandas as pd
import json
import logging
from datetime import datetime
from collections import defaultdict
from openai import AzureOpenAI
import time
import os
import hashlib
import re
from difflib import SequenceMatcher

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

"""
åŸºäºmedical_record_templatization_0806.pyçš„æ‰§è¡Œç»“æœï¼šDiseaseæè¿°åº“_ä¼˜åŒ–ç‰ˆ_20250806_121343.xlsx æ¥æ‰§è¡Œå¤„ç†

è¿›è¡Œç›¸ä¼¼çš„ç—‡çŠ¶å»é‡ ï¼Œ æ ¼å¼é‡ç»„

"""
class FastDescriptionDeduplicator:
    """
    å¿«é€Ÿæè¿°å»é‡å’Œæ ¼å¼åŒ–å·¥å…·
    è§„åˆ™ä¼˜å…ˆ + LLMè¾…åŠ© = 5-10å€é€Ÿåº¦æå‡
    """

    def __init__(self, azure_api_key, azure_endpoint, deployment_name="o3", cache_dir="cache"):
        """åˆå§‹åŒ–å¿«é€Ÿå»é‡å·¥å…·"""
        # ä¿®å¤endpointæ ¼å¼
        if azure_endpoint.endswith('chat/completions?'):
            azure_endpoint = azure_endpoint.replace('/openai/deployments/o3/chat/completions?', '')

        self.client = AzureOpenAI(
            api_key=azure_api_key,
            api_version="2025-01-01-preview",
            azure_endpoint=azure_endpoint
        )
        self.deployment_name = deployment_name

        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

        # 8ä¸ªå­—æ®µ
        self.key_fields = ['ä¸»è¯‰', 'ç°ç—…å²', 'æ—¢å¾€å²', 'è¾…åŠ©æ£€æŸ¥', 'PE/æ£€æŸ¥', 'ç—…æœº', 'æ²»åˆ™/å¤„ç†', 'åŒ»å˜±']

        # ç»Ÿè®¡
        self.api_calls = 0
        self.cache_hits = 0
        self.rule_processed = 0
        self.llm_processed = 0

    def get_cache_key(self, text):
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def load_cache(self, cache_file):
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        cache_path = os.path.join(self.cache_dir, cache_file)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_cache(self, cache_file, cache_data):
        """ä¿å­˜ç¼“å­˜æ–‡ä»¶"""
        cache_path = os.path.join(self.cache_dir, cache_file)
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

    def normalize_description(self, desc):
        """æ ‡å‡†åŒ–æè¿°ï¼ˆç”¨äºç›¸ä¼¼åº¦æ¯”è¾ƒï¼‰"""
        if not desc:
            return ""

        # æ—¶é—´æ ‡å‡†åŒ–
        normalized = re.sub(r'[0-9]+[å¤©æœˆå¹´å‘¨æ—¥]', 'Nå¤©', desc)
        normalized = re.sub(r'æ•°[å¤©æœˆå¹´å‘¨æ—¥]', 'Nå¤©', normalized)
        normalized = re.sub(r'å¤š[å¤©æœˆå¹´å‘¨æ—¥]', 'Nå¤©', normalized)
        normalized = re.sub(r'ä½™[å¤©æœˆå¹´å‘¨æ—¥]', 'Nå¤©', normalized)

        # æ•°å­—æ ‡å‡†åŒ–
        normalized = re.sub(r'[0-9]+[ä¸ªæ¬¡å›åº¦]', 'Nä¸ª', normalized)
        normalized = re.sub(r'ç¬¬[0-9]+', 'ç¬¬N', normalized)

        # å»é™¤æ ‡ç‚¹å’Œç©ºæ ¼
        normalized = re.sub(r'[ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿ\s]', '', normalized)

        return normalized.lower()

    def calculate_similarity(self, desc1, desc2):
        """è®¡ç®—ä¸¤ä¸ªæè¿°çš„ç›¸ä¼¼åº¦"""
        norm1 = self.normalize_description(desc1)
        norm2 = self.normalize_description(desc2)

        if not norm1 or not norm2:
            return 0.0

        # ä½¿ç”¨åºåˆ—åŒ¹é…å™¨è®¡ç®—ç›¸ä¼¼åº¦
        return SequenceMatcher(None, norm1, norm2).ratio()

    def is_containment_relation(self, desc1, desc2):
        """åˆ¤æ–­æ˜¯å¦ä¸ºåŒ…å«å…³ç³»"""
        norm1 = self.normalize_description(desc1)
        norm2 = self.normalize_description(desc2)

        if not norm1 or not norm2:
            return False, None

        # æ£€æŸ¥åŒ…å«å…³ç³»
        if norm1 in norm2:
            return True, desc1  # desc1 æ›´ç®€æ´
        elif norm2 in norm1:
            return True, desc2  # desc2 æ›´ç®€æ´

        return False, None

    def rule_based_grouping(self, descriptions):
        """åŸºäºè§„åˆ™çš„å¿«é€Ÿåˆ†ç»„"""
        if len(descriptions) <= 1:
            return [[desc] for desc in descriptions]

        groups = []
        processed = set()

        for i, desc1 in enumerate(descriptions):
            if desc1 in processed:
                continue

            current_group = [desc1]
            processed.add(desc1)

            # æ‰¾ç›¸ä¼¼å’ŒåŒ…å«çš„æè¿°
            for j, desc2 in enumerate(descriptions):
                if i == j or desc2 in processed:
                    continue

                # æ£€æŸ¥åŒ…å«å…³ç³»
                is_contain, shorter = self.is_containment_relation(desc1, desc2)
                if is_contain:
                    current_group.append(desc2)
                    processed.add(desc2)
                    continue

                # æ£€æŸ¥ç›¸ä¼¼åº¦
                similarity = self.calculate_similarity(desc1, desc2)
                if similarity > 0.85:  # é«˜ç›¸ä¼¼åº¦é˜ˆå€¼
                    current_group.append(desc2)
                    processed.add(desc2)

            groups.append(current_group)

        return groups

    def merge_similar_group(self, group):
        """åˆå¹¶ç›¸ä¼¼æè¿°ç»„"""
        if len(group) == 1:
            return group[0]

        # æ‰¾æœ€çŸ­çš„ä½œä¸ºåŸºç¡€ï¼ˆé€šå¸¸æœ€ç®€æ´ï¼‰
        base_desc = min(group, key=len)

        # æå–æ‰€æœ‰ç‹¬ç‰¹çš„å…³é”®è¯
        all_parts = set()
        locations = set()  # éƒ¨ä½è¯
        symptoms = set()  # ç—‡çŠ¶è¯

        location_patterns = ['é¢ˆ', 'è‚©', 'è…°', 'è†', 'å¤´', 'èƒ¸', 'è…¹', 'èƒŒ', 'è‡€', 'è…¿', 'æ‰‹', 'è¶³']
        symptom_patterns = ['ç–¼ç—›', 'ä¸é€‚', 'é…¸èƒ€', 'éº»æœ¨', 'åƒµç¡¬', 'å¤´ç—›', 'å¤´æ™•', 'ä¹åŠ›', 'ç–²åŠ³']

        for desc in group:
            # æå–éƒ¨ä½
            for loc in location_patterns:
                if loc in desc:
                    locations.add(loc)

            # æå–ç—‡çŠ¶
            for symp in symptom_patterns:
                if symp in desc:
                    symptoms.add(symp)

        # æ™ºèƒ½åˆå¹¶
        if len(locations) > 1:
            # å¤šä¸ªéƒ¨ä½ï¼šé¢ˆ[è‚©/è…°]ç–¼ç—›
            loc_str = '[' + '/'.join(sorted(locations)) + ']'
            if len(symptoms) > 1:
                symp_str = '/'.join(sorted(symptoms))
                return f"{loc_str}{symp_str}Nå¤©"
            elif symptoms:
                return f"{loc_str}{list(symptoms)[0]}Nå¤©"

        if len(symptoms) > 1:
            # å¤šä¸ªç—‡çŠ¶ï¼šç–¼ç—›/ä¸é€‚
            return '/'.join(sorted(symptoms)) + 'Nå¤©'

        # é»˜è®¤è¿”å›æœ€çŸ­çš„æè¿°
        return self.normalize_time_expression(base_desc)

    def normalize_time_expression(self, desc):
        """æ ‡å‡†åŒ–æ—¶é—´è¡¨è¾¾"""
        # ç»Ÿä¸€ä¸ºNå¤©
        desc = re.sub(r'[0-9]+ä¸ª?[å¤©æœˆå¹´å‘¨æ—¥]', 'Nå¤©', desc)
        desc = re.sub(r'æ•°[å¤©æœˆå¹´å‘¨æ—¥]', 'Nå¤©', desc)
        desc = re.sub(r'å¤š[å¤©æœˆå¹´å‘¨æ—¥]', 'Nå¤©', desc)
        desc = re.sub(r'ä½™[å¤©æœˆå¹´å‘¨æ—¥]', 'Nå¤©', desc)
        desc = re.sub(r'[å¤©æœˆå¹´å‘¨æ—¥]ä½™', 'Nå¤©', desc)

        return desc

    def need_llm_processing(self, groups, original_count):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦LLMå¤„ç†"""
        # å¦‚æœè§„åˆ™å·²ç»è¾¾åˆ°å¾ˆå¥½çš„å»é‡æ•ˆæœï¼Œå°±ä¸ç”¨LLM
        merged_count = len(groups)
        reduction_rate = (original_count - merged_count) / original_count

        # å¦‚æœå»é‡ç‡å·²ç»è¶…è¿‡30%ï¼Œæˆ–è€…å‰©ä½™ç»„æ•°å¾ˆå°‘ï¼Œå°±ä¸ç”¨LLM
        if reduction_rate > 0.3 or merged_count <= 3:
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰å¤æ‚æƒ…å†µéœ€è¦LLM
        complex_groups = 0
        for group in groups:
            if len(group) > 1:
                # æ£€æŸ¥ç»„å†…æ˜¯å¦è¿˜æœ‰å¯ä»¥è¿›ä¸€æ­¥åˆå¹¶çš„
                for i, desc1 in enumerate(group):
                    for j, desc2 in enumerate(group[i + 1:], i + 1):
                        sim = self.calculate_similarity(desc1, desc2)
                        if 0.6 < sim <= 0.85:  # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼Œéœ€è¦LLMåˆ¤æ–­
                            complex_groups += 1
                            break
                    if complex_groups > 0:
                        break

        return complex_groups > 0

    def call_azure_api(self, prompt, max_retries=3):
        """è°ƒç”¨Azure OpenAI APIï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self.get_cache_key(prompt)
        cache_file = "fast_dedup_api_responses.json"
        cache_data = self.load_cache(cache_file)

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in cache_data:
            self.cache_hits += 1
            return cache_data[cache_key]

        # APIè°ƒç”¨
        for attempt in range(max_retries):
            try:
                self.api_calls += 1
                response = self.client.chat.completions.create(
                    model=self.deployment_name,
                    messages=[
                        {"role": "system",
                         "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—ä¿¡æ¯å¤„ç†ä¸“å®¶ï¼Œæ“…é•¿å¿«é€Ÿè¯†åˆ«å’Œåˆå¹¶ç›¸ä¼¼çš„åŒ»ç–—æè¿°ã€‚è¯·ç®€æ´å›ç­”ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                )

                if response.choices and response.choices[0].message and response.choices[0].message.content:
                    result = response.choices[0].message.content.strip()
                    # ä¿å­˜åˆ°ç¼“å­˜
                    cache_data[cache_key] = result
                    self.save_cache(cache_file, cache_data)
                    return result
                else:
                    logger.warning(f"APIè¿”å›ç©ºå“åº” (å°è¯• {attempt + 1}/{max_retries})")

            except Exception as e:
                logger.warning(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(1)  # ç¼©çŸ­ç­‰å¾…æ—¶é—´

        logger.error("APIè°ƒç”¨æœ€ç»ˆå¤±è´¥")
        return None

    def llm_fine_tune(self, field_name, groups):
        """LLMç²¾ç»†è°ƒæ•´ï¼ˆåªå¤„ç†å¤æ‚æƒ…å†µï¼‰"""
        if len(groups) <= 1:
            return [self.merge_similar_group(group) for group in groups]

        # åªå‘é€éœ€è¦LLMåˆ¤æ–­çš„ç»„
        complex_groups_text = []
        simple_results = []

        for i, group in enumerate(groups):
            if len(group) == 1:
                # å•ä¸ªæè¿°ï¼Œç›´æ¥ä¿ç•™
                simple_results.append((i, group[0]))
            else:
                # å¤šä¸ªæè¿°ï¼Œéœ€è¦LLMåˆ¤æ–­å¦‚ä½•åˆå¹¶
                group_text = f"ç»„{i + 1}: {' | '.join(group)}"
                complex_groups_text.append(group_text)

        if not complex_groups_text:
            # æ²¡æœ‰å¤æ‚ç»„ï¼Œç›´æ¥è¿”å›è§„åˆ™ç»“æœ
            return [group[0] for group in groups]

        # æ„å»ºç®€åŒ–çš„LLM prompt
        prompt = f"""
å¿«é€Ÿåˆå¹¶ä»¥ä¸‹{field_name}å­—æ®µçš„ç›¸ä¼¼æè¿°ç»„ï¼Œæ¯ç»„åˆå¹¶ä¸º1ä¸ªæè¿°ï¼š

{chr(10).join(complex_groups_text)}

è§„åˆ™ï¼š
1. ç›¸ä¼¼ç—‡çŠ¶ç”¨/è¿æ¥ï¼šç–¼ç—›/ä¸é€‚
2. å¤šéƒ¨ä½ç”¨[]ï¼šé¢ˆ[è‚©/é¡¹]
3. æ—¶é—´ç»Ÿä¸€ï¼šNå¤©
4. ä¿æŒç®€æ´

JSONæ ¼å¼è¿”å›ï¼š["åˆå¹¶åæè¿°1", "åˆå¹¶åæè¿°2", ...]
"""

        response = self.call_azure_api(prompt)
        if response:
            try:
                llm_results = json.loads(response)
                if isinstance(llm_results, list):
                    # åˆå¹¶LLMç»“æœå’Œç®€å•ç»“æœ
                    final_results = []
                    llm_index = 0

                    for i, group in enumerate(groups):
                        if len(group) == 1:
                            final_results.append(group[0])
                        else:
                            if llm_index < len(llm_results):
                                final_results.append(llm_results[llm_index])
                                llm_index += 1
                            else:
                                # LLMç»“æœä¸å¤Ÿï¼Œä½¿ç”¨è§„åˆ™åˆå¹¶
                                final_results.append(self.merge_similar_group(group))

                    return final_results
            except json.JSONDecodeError:
                logger.warning(f"{field_name}å­—æ®µLLMç»“æœè§£æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åˆå¹¶")

        # å¤‡ç”¨æ–¹æ¡ˆï¼šè§„åˆ™åˆå¹¶
        return [self.merge_similar_group(group) for group in groups]

    def fast_deduplicate_field(self, field_name, descriptions):
        """å¿«é€Ÿå»é‡å•ä¸ªå­—æ®µï¼ˆè§„åˆ™ä¼˜å…ˆ+LLMè¾…åŠ©ï¼‰"""
        if len(descriptions) <= 1:
            return descriptions

        original_count = len(descriptions)

        # ç¬¬1æ­¥ï¼šè§„åˆ™å¿«é€Ÿåˆ†ç»„ï¼ˆ90%æƒ…å†µå¤„ç†å®Œï¼‰
        groups = self.rule_based_grouping(descriptions)

        # ç¬¬2æ­¥ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦LLMç²¾ç»†å¤„ç†
        if self.need_llm_processing(groups, original_count):
            # éœ€è¦LLMå¤„ç†å¤æ‚æƒ…å†µ
            result = self.llm_fine_tune(field_name, groups)
            self.llm_processed += 1
        else:
            # è§„åˆ™å¤„ç†å·²è¶³å¤Ÿ
            result = [self.merge_similar_group(group) for group in groups]
            self.rule_processed += 1

        # å»é™¤ç©ºå€¼å’Œé‡å¤
        result = list(filter(None, list(dict.fromkeys(result))))

        return result

    def process_disease_data(self, df):
        """
        å¿«é€Ÿå¤„ç†Diseaseæ•°æ®å»é‡
        """
        logger.info("å¼€å§‹å¿«é€Ÿå»é‡å¤„ç†...")

        # æŒ‰Diseaseå’Œå­—æ®µåˆ†ç»„
        disease_field_data = defaultdict(lambda: defaultdict(list))

        for _, row in df.iterrows():
            disease = row['Disease']
            field = row['æ¥æºå­—æ®µ']
            description = row['æè¿°å†…å®¹']

            if description and str(description).strip():
                disease_field_data[disease][field].append(str(description).strip())

        # å¿«é€Ÿå¤„ç†æ¯ä¸ªDisease
        processed_data = {}
        total_diseases = len(disease_field_data)

        for i, (disease, field_data) in enumerate(disease_field_data.items(), 1):
            logger.info(f"å¿«é€Ÿå¤„ç†Disease {i}/{total_diseases}: {disease}")
            processed_data[disease] = {}

            for field in self.key_fields:
                descriptions = field_data.get(field, [])
                if descriptions:
                    # å»é‡å‰åæ•°é‡
                    before_count = len(descriptions)
                    deduplicated = self.fast_deduplicate_field(field, descriptions)
                    after_count = len(deduplicated)

                    processed_data[disease][field] = deduplicated

                    reduction = before_count - after_count
                    if reduction > 0:
                        logger.info(f"  {field}: {before_count} â†’ {after_count} (-{reduction})")
                    else:
                        logger.info(f"  {field}: {before_count} (æ— å˜åŒ–)")
                else:
                    processed_data[disease][field] = []

            # å‡å°‘å»¶æ—¶
            if i < total_diseases:
                time.sleep(0.1)  # åªåœ¨æœ‰LLMè°ƒç”¨æ—¶æ‰éœ€è¦å»¶æ—¶

        return processed_data

    def format_to_vertical_table(self, processed_data):
        """æ ¼å¼åŒ–ä¸ºçºµå‘è¡¨æ ¼"""
        logger.info("æ ¼å¼åŒ–ä¸ºçºµå‘è¡¨æ ¼...")

        all_rows = []

        for disease, field_data in processed_data.items():
            # æ‰¾åˆ°æœ€é•¿çš„å­—æ®µ
            max_length = max([len(descriptions) for descriptions in field_data.values()] + [0])

            if max_length == 0:
                continue

            # ä¸ºè¯¥Diseaseç”Ÿæˆå¤šè¡Œæ•°æ®
            for row_index in range(max_length):
                row = {'Disease': disease if row_index == 0 else ''}

                for field in self.key_fields:
                    descriptions = field_data.get(field, [])
                    if row_index < len(descriptions):
                        row[field] = descriptions[row_index]
                    else:
                        row[field] = ''

                all_rows.append(row)

        columns = ['Disease'] + self.key_fields
        df_result = pd.DataFrame(all_rows, columns=columns)

        logger.info(f"ç”Ÿæˆè¡¨æ ¼: {len(processed_data)} ä¸ªDisease, {len(all_rows)} è¡Œæ•°æ®")
        return df_result

    def run(self, input_file, output_file=None):
        """è¿è¡Œå¿«é€Ÿå»é‡å’Œæ ¼å¼åŒ–æµç¨‹"""
        start_time = time.time()

        logger.info("=" * 80)
        logger.info("å¼€å§‹å¿«é€ŸDiseaseæè¿°å»é‡å’Œæ ¼å¼åŒ–")
        logger.info("=" * 80)

        try:
            # è¯»å–åŸå§‹æ•°æ®
            logger.info(f"è¯»å–æ–‡ä»¶: {input_file}")
            df = pd.read_excel(input_file)
            logger.info(f"åŸå§‹æ•°æ®: {len(df)} æ¡æè¿°è®°å½•")

            # ç»Ÿè®¡åŸå§‹æ•°æ®
            disease_count = df['Disease'].nunique()
            field_stats = df['æ¥æºå­—æ®µ'].value_counts()
            logger.info(f"åŒ…å« {disease_count} ä¸ªDisease")

            # å¿«é€Ÿå¤„ç†æ•°æ®å»é‡
            processed_data = self.process_disease_data(df)

            # æ ¼å¼åŒ–ä¸ºçºµå‘è¡¨æ ¼
            df_formatted = self.format_to_vertical_table(processed_data)

            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            if not output_file:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = f"result_{timestamp}.xlsx"

            # ä¿å­˜ç»“æœ
            df_formatted.to_excel(output_file, index=False)

            # ç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            processing_time = end_time - start_time

            # è®¡ç®—å»é‡æ•ˆæœ
            total_original = len(df)
            total_after_dedup = sum(len(descriptions) for field_data in processed_data.values()
                                    for descriptions in field_data.values())
            dedup_rate = (total_original - total_after_dedup) / total_original * 100

            logger.info("=" * 80)
            logger.info("ğŸ“Š å¿«é€Ÿå»é‡å®Œæˆç»Ÿè®¡")
            logger.info("=" * 80)
            logger.info(f"å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’ âš¡")
            logger.info(f"APIè°ƒç”¨æ¬¡æ•°: {self.api_calls}")
            logger.info(f"ç¼“å­˜å‘½ä¸­æ¬¡æ•°: {self.cache_hits}")
            logger.info(f"è§„åˆ™å¤„ç†å­—æ®µæ•°: {self.rule_processed}")
            logger.info(f"LLMå¤„ç†å­—æ®µæ•°: {self.llm_processed}")
            logger.info(f"è§„åˆ™å¤„ç†ç‡: {self.rule_processed / (self.rule_processed + self.llm_processed) * 100:.1f}%")
            logger.info(f"åŸå§‹æè¿°æ•°: {total_original}")
            logger.info(f"å»é‡åæè¿°æ•°: {total_after_dedup}")
            logger.info(f"å»é‡ç‡: {dedup_rate:.1f}%")
            logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
            logger.info(f"è¾“å‡ºæ ¼å¼: {len(processed_data)} ä¸ªDisease, {len(df_formatted)} è¡Œ")
            logger.info("=" * 80)

        except Exception as e:
            logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    # Azure OpenAIé…ç½®
    AZURE_API_KEY = ""
    AZURE_ENDPOINT = ""
    DEPLOYMENT_NAME = "o3"

    # è¾“å…¥è¾“å‡ºæ–‡ä»¶
    input_file = "Diseaseæè¿°åº“_ä¼˜åŒ–ç‰ˆ_20250806_121343.xlsx"

    # åˆ›å»ºå¿«é€Ÿå»é‡å·¥å…·å¹¶è¿è¡Œ
    deduplicator = FastDescriptionDeduplicator(
        azure_api_key=AZURE_API_KEY,
        azure_endpoint=AZURE_ENDPOINT,
        deployment_name=DEPLOYMENT_NAME
    )

    # è¿è¡Œå¿«é€Ÿå»é‡å’Œæ ¼å¼åŒ–
    deduplicator.run(input_file)


if __name__ == "__main__":
    main()
