import pandas as pd
import json
import logging
import asyncio
from datetime import datetime
from collections import defaultdict
from openai import AzureOpenAI
import time
import os
import hashlib
from tqdm import tqdm
from difflib import SequenceMatcher
import re
import threading
"""
æ ¹æ®ã€Šè¯Šç–—è§„èŒƒã€‹å†è·‘ä¸€ç‰ˆåæ­£éªŒè¯çš„ç»“æœï¼Œè¿™ä¸€ç‰ˆæ²¡æœ‰ç½®ä¿¡åº¦ï¼Œè¦ç»™å‡ºä¸é€šè¿‡çš„ç†ç”±ã€‚

ä¸»è¦ç»“æœæ–‡ä»¶: ../data/case_data\åå‘éªŒè¯2kg_20250810_230721.xlsx
è¿‡ç¨‹è®°å½•æ–‡ä»¶: ../data/case_data\éªŒè¯è¿‡ç¨‹è®°å½•_20250810_230721.xlsx
è¾“å‡ºæ ¼å¼è¯´æ˜:
ä¸»è¦ç»“æœ: Disease | å­—æ®µ | æè¿°å†…å®¹ | æ˜¯å¦é€šè¿‡ | ä¸é€šè¿‡ç†ç”±
è¿‡ç¨‹è®°å½•: åŒ…å«ç–¾ç—…åŒ¹é…ã€çŸ¥è¯†åº“ä½¿ç”¨ã€å®Œæ•´promptç­‰è¯¦ç»†ä¿¡æ¯


ç»“æœï¼š
å¤„ç†ç–¾ç—…æ•°: 176
éªŒè¯æè¿°æ€»æ•°: 3197
é€šè¿‡: 1415 (44.3%)
ä¸é€šè¿‡: 1782 (55.7%)
"""
# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.FileHandler('reverse_validation_2kg.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class KnowledgeBasedValidator:
    """
    åŸºäºçŸ¥è¯†åº“çš„åŒ»ç–—æè¿°å®¡æ ¸ç³»ç»Ÿ
    æ”¯æŒè¯¦ç»†æ—¥å¿—è®°å½•ã€ä¸­é—´ç»“æœè¿½æº¯
    """

    def __init__(self, azure_api_key, azure_endpoint, deployment_name="o3",
                 cache_dir="cache", knowledge_base_path="../../data/other/book_structured_enhanced.json"):
        """
        åˆå§‹åŒ–å®¡æ ¸ç³»ç»Ÿ
        """
        logger.info("ğŸš€ åˆå§‹åŒ–åŸºäºçŸ¥è¯†åº“çš„åŒ»ç–—æè¿°å®¡æ ¸ç³»ç»Ÿ")

        # ä¿®å¤endpointæ ¼å¼
        if azure_endpoint.endswith('chat/completions?'):
            azure_endpoint = azure_endpoint.replace('/openai/deployments/o3/chat/completions?', '')

        self.client = AzureOpenAI(
            api_key=azure_api_key,
            api_version="2025-01-01-preview",
            azure_endpoint=azure_endpoint
        )
        self.deployment_name = deployment_name

        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
        logger.info(f"ğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}")

        # å®šä¹‰åŒ»ç–—å­—æ®µ
        self.key_fields = ['ä¸»è¯‰', 'ç°ç—…å²', 'æ—¢å¾€å²', 'è¾…åŠ©æ£€æŸ¥', 'PE/æ£€æŸ¥ ï¼ˆä½“ç°æœ›é—»é—®åˆ‡ï¼‰', 'ç—…æœº', 'æ²»åˆ™/å¤„ç†',
                           'åŒ»å˜±']

        # ç»Ÿè®¡ä¿¡æ¯
        self.api_calls = 0
        self.cache_hits = 0
        self.processing_records = []  # ä¸­é—´ç»“æœè®°å½•

        # åŠ è½½çŸ¥è¯†åº“
        self.knowledge_base = self.load_knowledge_base(knowledge_base_path)

    def load_knowledge_base(self, file_path):
        """
        åŠ è½½å¹¶é¢„å¤„ç†çŸ¥è¯†åº“
        """
        logger.info(f"ğŸ“š å¼€å§‹åŠ è½½çŸ¥è¯†åº“: {file_path}")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_kb = json.load(f)

            # é¢„å¤„ç†çŸ¥è¯†åº“ï¼Œå»ºç«‹ç–¾ç—…ç´¢å¼•
            processed_kb = {}
            disease_count = 0

            if 'book' in raw_kb and 'disciplines' in raw_kb['book']:
                for discipline in raw_kb['book']['disciplines']:
                    discipline_name = discipline.get('name', 'æœªçŸ¥ç§‘å®¤')
                    logger.info(f"  å¤„ç†ç§‘å®¤: {discipline_name}")

                    for item in discipline.get('items', []):
                        disease_name = item.get('name', '')
                        if disease_name:
                            processed_kb[disease_name] = {
                                'discipline': discipline_name,
                                'sections': item.get('sections', {}),
                                'number': item.get('number', '')
                            }
                            disease_count += 1

            logger.info(f"âœ… çŸ¥è¯†åº“åŠ è½½å®Œæˆ: {disease_count} ä¸ªç–¾ç—…")

            # è¾“å‡ºç–¾ç—…åˆ—è¡¨
            diseases = list(processed_kb.keys())
            logger.info(f"ğŸ“‹ çŸ¥è¯†åº“ç–¾ç—…åˆ—è¡¨(å‰10ä¸ª): {diseases[:10]}")

            return processed_kb

        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†åº“åŠ è½½å¤±è´¥: {str(e)}")
            return {}

    def normalize_disease_name(self, disease_name):
        """
        æ ‡å‡†åŒ–ç–¾ç—…åç§°
        """
        if not disease_name:
            return ""

        # ç§»é™¤å¸¸è§åç¼€å’Œç‰¹æ®Šå­—ç¬¦
        normalized = disease_name.strip()
        normalized = re.sub(r'[ï¼ˆï¼‰\(\)\[\]ã€ã€‘\s]+', '', normalized)

        # ç§»é™¤å¸¸è§å‰ç¼€ï¼ˆå¦‚ï¼šæ··åˆå‹ã€æ…¢æ€§ã€æ€¥æ€§ç­‰ï¼‰
        prefixes = ['æ··åˆå‹', 'æ…¢æ€§', 'æ€¥æ€§', 'å¤å‘æ€§', 'åŸå‘æ€§', 'ç»§å‘æ€§', 'é¡½å›ºæ€§']
        for prefix in prefixes:
            if normalized.startswith(prefix):
                normalized = normalized[len(prefix):]
                break

        # ç§»é™¤å¸¸è§åç¼€
        suffixes = ['ç—…', 'ç—‡', 'è¯', 'ç»¼åˆå¾', 'ç»¼åˆç—‡']
        for suffix in suffixes:
            if normalized.endswith(suffix) and len(normalized) > len(suffix):
                normalized = normalized[:-len(suffix)]
                break

        return normalized.lower()

    def match_disease_to_knowledge(self, excel_disease):
        """
        å°†Excelä¸­çš„ç–¾ç—…ååŒ¹é…åˆ°çŸ¥è¯†åº“
        """
        logger.debug(f"ğŸ” å¼€å§‹åŒ¹é…ç–¾ç—…: {excel_disease}")

        kb_diseases = list(self.knowledge_base.keys())

        # 1. ç²¾ç¡®åŒ¹é…
        if excel_disease in kb_diseases:
            logger.debug(f"âœ… ç²¾ç¡®åŒ¹é…æˆåŠŸ: {excel_disease}")
            return excel_disease, "ç²¾ç¡®åŒ¹é…", 1.0

        # 2. æ ‡å‡†åŒ–ååŒ¹é…
        normalized_excel = self.normalize_disease_name(excel_disease)
        for kb_disease in kb_diseases:
            if normalized_excel == self.normalize_disease_name(kb_disease):
                logger.debug(f"âœ… æ ‡å‡†åŒ–åŒ¹é…æˆåŠŸ: {excel_disease} -> {kb_disease}")
                return kb_disease, "æ ‡å‡†åŒ–åŒ¹é…", 0.9

        # 3. åŒ…å«å…³ç³»åŒ¹é…
        best_match = None
        best_score = 0
        for kb_disease in kb_diseases:
            # æ£€æŸ¥åŒ…å«å…³ç³»
            if normalized_excel in self.normalize_disease_name(kb_disease):
                score = len(normalized_excel) / len(self.normalize_disease_name(kb_disease))
                if score > best_score:
                    best_match = kb_disease
                    best_score = score
            elif self.normalize_disease_name(kb_disease) in normalized_excel:
                score = len(self.normalize_disease_name(kb_disease)) / len(normalized_excel)
                if score > best_score:
                    best_match = kb_disease
                    best_score = score

        if best_match and best_score > 0.5:
            logger.debug(f"âœ… åŒ…å«åŒ¹é…æˆåŠŸ: {excel_disease} -> {best_match} (å¾—åˆ†: {best_score:.2f})")
            return best_match, "åŒ…å«åŒ¹é…", best_score

        # 4. ç›¸ä¼¼åº¦åŒ¹é…
        best_match = None
        best_score = 0
        for kb_disease in kb_diseases:
            similarity = SequenceMatcher(None, normalized_excel,
                                         self.normalize_disease_name(kb_disease)).ratio()
            if similarity > best_score:
                best_match = kb_disease
                best_score = similarity

        if best_match and best_score > 0.6:
            logger.debug(f"âœ… ç›¸ä¼¼åº¦åŒ¹é…æˆåŠŸ: {excel_disease} -> {best_match} (å¾—åˆ†: {best_score:.2f})")
            return best_match, "ç›¸ä¼¼åº¦åŒ¹é…", best_score

        # åŒ¹é…å¤±è´¥
        logger.warning(f"âŒ ç–¾ç—…åŒ¹é…å¤±è´¥: {excel_disease}")
        return None, "åŒ¹é…å¤±è´¥", 0.0

    def extract_relevant_knowledge(self, disease_name, field_type):
        """
        æ ¹æ®å­—æ®µç±»å‹æå–ç›¸å…³çŸ¥è¯†
        """
        if disease_name not in self.knowledge_base:
            return "", []

        disease_info = self.knowledge_base[disease_name]
        sections = disease_info.get('sections', {})

        # å­—æ®µä¸çŸ¥è¯†åº“ç« èŠ‚çš„æ˜ å°„å…³ç³»
        field_mapping = {
            "ä¸»è¯‰": ["æ­£æ–‡", "è¯Šæ–­ä¾æ®"],
            "ç°ç—…å²": ["æ­£æ–‡", "è¯Šæ–­ä¾æ®", "è¯å€™åˆ†ç±»"],
            "æ—¢å¾€å²": ["è¯Šæ–­ä¾æ®", "å¹¶å‘ç—‡å¤„ç†"],
            "è¾…åŠ©æ£€æŸ¥": ["è¯Šæ–­ä¾æ®"],
            "PE/æ£€æŸ¥ ï¼ˆä½“ç°æœ›é—»é—®åˆ‡ï¼‰": ["è¯Šæ–­ä¾æ®", "è¯å€™åˆ†ç±»"],
            "ç—…æœº": ["è¯å€™åˆ†ç±»"],
            "æ²»åˆ™/å¤„ç†": ["æ²»ç–—æ–¹æ¡ˆ", "å…¶ä»–ç–—æ³•"],
            "åŒ»å˜±": ["å…¶ä»–ç–—æ³•", "å¹¶å‘ç—‡å¤„ç†"]
        }

        relevant_sections = field_mapping.get(field_type, ["æ­£æ–‡"])
        knowledge_parts = []
        used_sections = []

        for section_name in relevant_sections:
            if section_name in sections:
                section_content = sections[section_name]
                if isinstance(section_content, list):
                    content = '\n'.join(section_content)
                else:
                    content = str(section_content)

                if content.strip():
                    knowledge_parts.append(f"ã€{section_name}ã€‘\n{content}")
                    used_sections.append(section_name)

        knowledge_text = '\n\n'.join(knowledge_parts)
        logger.debug(f"ğŸ“– æå–çŸ¥è¯† - ç–¾ç—…: {disease_name}, å­—æ®µ: {field_type}, ä½¿ç”¨ç« èŠ‚: {used_sections}")

        return knowledge_text, used_sections

    def clean_json_response(self, response_text):
        """
        æ¸…ç†å’Œä¿®å¤APIè¿”å›çš„JSONå“åº”
        """
        if not response_text:
            return response_text

        # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        response_text = re.sub(r'```json\s*\n?', '', response_text)
        response_text = re.sub(r'```\s*$', '', response_text)

        # å¤„ç†å¤šè¡Œå­—ç¬¦ä¸²ä¸­çš„æ¢è¡Œç¬¦
        def escape_newlines_in_strings(match):
            content = match.group(1)
            # è½¬ä¹‰å­—ç¬¦ä¸²å†…çš„æ¢è¡Œç¬¦å’Œå¼•å·
            content = content.replace('\n', '\\n').replace('\r', '\\r').replace('"', '\\"')
            return f'"{content}"'

        # ä¿®å¤å­—ç¬¦ä¸²å€¼ä¸­çš„æ¢è¡Œç¬¦
        response_text = re.sub(r'"([^"]*(?:\n[^"]*)*)"', escape_newlines_in_strings, response_text)

        # ç§»é™¤æ³¨é‡Šï¼ˆå¦‚æœæœ‰ï¼‰
        response_text = re.sub(r'//.*$', '', response_text, flags=re.MULTILINE)

        return response_text.strip()

    def parse_response_backup(self, response, descriptions):
        """
        å¤‡ç”¨å“åº”è§£ææ–¹æ³•
        """
        try:
            results = []

            # æŸ¥æ‰¾é€šè¿‡/ä¸é€šè¿‡çš„æ¨¡å¼
            for i, description in enumerate(descriptions, 1):
                # æŸ¥æ‰¾ç¼–å·å¯¹åº”çš„ç»“æœ
                pattern = rf'"{i}".*?"result".*?"(é€šè¿‡|ä¸é€šè¿‡)".*?"reason".*?"([^"]*)"'
                match = re.search(pattern, response, re.DOTALL)

                if match:
                    result = match.group(1)
                    reason = match.group(2)
                    results.append({
                        'result': result,
                        'reason': reason
                    })
                else:
                    # æ‰¾ä¸åˆ°åŒ¹é…ï¼Œé»˜è®¤é€šè¿‡
                    results.append({
                        'result': 'é€šè¿‡',
                        'reason': 'å¤‡ç”¨è§£ææ–¹æ³•ï¼šæœªæ‰¾åˆ°æ˜ç¡®ç»“æœï¼Œé»˜è®¤é€šè¿‡'
                    })

            return results if len(results) == len(descriptions) else None

        except Exception as e:
            logger.error(f"å¤‡ç”¨è§£ææ–¹æ³•ä¹Ÿå¤±è´¥: {str(e)}")
            return None

    def validate_single_disease_batch(self, disease_data):
        """
        éªŒè¯å•ä¸ªç–¾ç—…çš„æ‰€æœ‰æè¿°
        """
        disease = disease_data['disease']
        items = disease_data['items']

        logger.info(f"ğŸ”¬ å¼€å§‹éªŒè¯ç–¾ç—…: {disease} ({len(items)} æ¡æè¿°)")

        # ç–¾ç—…ååŒ¹é…
        matched_disease, match_method, match_score = self.match_disease_to_knowledge(disease)

        if not matched_disease:
            logger.warning(f"âš ï¸ ç–¾ç—… {disease} æœªæ‰¾åˆ°åŒ¹é…çš„çŸ¥è¯†åº“æ¡ç›®")
            # è¿”å›æ‰€æœ‰æè¿°ä¸ºä¸é€šè¿‡
            results = []
            for item in items:
                results.append({
                    'Disease': disease,
                    'å­—æ®µ': item['field'],
                    'æè¿°å†…å®¹': item['description'],
                    'æ˜¯å¦é€šè¿‡': 'ä¸é€šè¿‡',
                    'ä¸é€šè¿‡ç†ç”±': f'çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç–¾ç—… "{disease}" çš„ç›¸å…³ä¿¡æ¯ï¼Œå»ºè®®æ£€æŸ¥ç–¾ç—…åç§°æˆ–è¡¥å……çŸ¥è¯†åº“'
                })
            return results

        logger.info(f"âœ… ç–¾ç—…åŒ¹é…æˆåŠŸ: {disease} -> {matched_disease} ({match_method}, å¾—åˆ†: {match_score:.2f})")

        # æŒ‰å­—æ®µåˆ†ç»„æè¿°
        field_groups = defaultdict(list)
        for item in items:
            field_groups[item['field']].append(item['description'])

        # ä¸ºæ¯ä¸ªå­—æ®µæ„å»ºæ‰¹é‡éªŒè¯prompt
        all_results = []
        for field, descriptions in field_groups.items():
            logger.info(f"  ğŸ” éªŒè¯å­—æ®µ: {field} ({len(descriptions)} æ¡æè¿°)")

            # æå–ç›¸å…³çŸ¥è¯†
            knowledge_text, used_sections = self.extract_relevant_knowledge(matched_disease, field)

            # æ„å»ºéªŒè¯é¡¹ç›®åˆ—è¡¨
            items_text = []
            for i, desc in enumerate(descriptions, 1):
                items_text.append(f"{i}. {desc}")

            # ä¼˜åŒ–åçš„promptï¼Œæ›´åŠ å®½æ¾åˆç†
            prompt = f"""
ä½ æ˜¯æƒå¨çš„ä¸­åŒ»ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹æ ‡å‡†åŒ»å­¦çŸ¥è¯†åº“å®¡æ ¸åŒ»ç–—æè¿°çš„åˆç†æ€§ã€‚

ã€å®¡æ ¸ç–¾ç—…ã€‘: {matched_disease}
ã€å®¡æ ¸å­—æ®µã€‘: {field}
ã€æ ‡å‡†çŸ¥è¯†åº“å†…å®¹ã€‘:
{knowledge_text if knowledge_text else "è¯¥å­—æ®µæš‚æ— å¯¹åº”çš„æ ‡å‡†çŸ¥è¯†åº“å†…å®¹ï¼Œè¯·åŸºäºä¸­åŒ»ç†è®ºå’Œä¸´åºŠå¸¸è¯†åˆ¤æ–­"}

ã€å¾…å®¡æ ¸æè¿°åˆ—è¡¨ã€‘:
{chr(10).join(items_text)}

ã€å®¡æ ¸åŸåˆ™ã€‘:
1. å¦‚æœçŸ¥è¯†åº“æœ‰æ˜ç¡®å†…å®¹ï¼Œä¼˜å…ˆå‚è€ƒçŸ¥è¯†åº“
2. å¦‚æœçŸ¥è¯†åº“å†…å®¹æœ‰é™ï¼ŒåŸºäºä¸­åŒ»ç†è®ºå’Œä¸´åºŠå¸¸è¯†åˆ¤æ–­
3. é‡‡ç”¨ç›¸å¯¹å®½æ¾çš„æ ‡å‡†ï¼Œåªè¦ä¸æ˜¯æ˜æ˜¾é”™è¯¯å°±é€šè¿‡
4. è€ƒè™‘ä¸­åŒ»çš„æ•´ä½“è§‚å¿µå’Œè¾¨è¯è®ºæ²»ç‰¹è‰²

ã€åˆ¤æ–­æ ‡å‡†ã€‘:
- é€šè¿‡ï¼šæè¿°ç¬¦åˆè¯¥ç–¾ç—…çš„å¯èƒ½è¡¨ç°ï¼Œæˆ–ä¸ä¸­åŒ»ç†è®ºç›¸ç¬¦
- ä¸é€šè¿‡ï¼šæè¿°æ˜æ˜¾é”™è¯¯ï¼Œä¸è¯¥ç–¾ç—…å®Œå…¨æ— å…³ï¼Œæˆ–å­˜åœ¨ä¸¥é‡åŒ»å­¦é”™è¯¯

ã€è¾“å‡ºè¦æ±‚ã€‘:
- å¿…é¡»è¿”å›æ ‡å‡†JSONæ ¼å¼
- æ¯ä¸ªç¼–å·å¯¹åº”ä¸€ä¸ªç»“æœ
- ç†ç”±è¦å…·ä½“æ˜ç¡®

è¿”å›æ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼Œä¸è¦é¢å¤–è¯´æ˜ï¼‰:
{{
  "1": {{"result": "é€šè¿‡", "reason": "ç¬¦åˆè¯¥ç–¾ç—…çš„ä¸´åºŠè¡¨ç°"}},
  "2": {{"result": "ä¸é€šè¿‡", "reason": "å…·ä½“ä¸é€šè¿‡çš„åŸå› "}}
}}
"""

            # è®°å½•å¤„ç†è¿‡ç¨‹
            processing_record = {
                'timestamp': datetime.now().isoformat(),
                'excel_disease': disease,
                'matched_disease': matched_disease,
                'match_method': match_method,
                'match_score': match_score,
                'field': field,
                'descriptions_count': len(descriptions),
                'used_knowledge_sections': used_sections,
                'full_prompt': prompt,
                'knowledge_content': knowledge_text
            }
            self.processing_records.append(processing_record)

            # è°ƒç”¨API
            response = self.call_azure_api(prompt)

            # è§£æç»“æœï¼ˆå¢å¼ºå®¹é”™æ€§ï¼‰
            if response:
                try:
                    # æ¸…ç†JSONå“åº”
                    cleaned_response = self.clean_json_response(response)
                    validation_data = json.loads(cleaned_response)

                    for i, description in enumerate(descriptions, 1):
                        key = str(i)
                        if key in validation_data and isinstance(validation_data[key], dict):
                            result_data = validation_data[key]
                            is_passed = result_data.get('result', 'ä¸é€šè¿‡')
                            reason = result_data.get('reason', 'æ— æ˜ç¡®ç†ç”±')
                        else:
                            is_passed = 'ä¸é€šè¿‡'
                            reason = f'APIå“åº”æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ç¼–å·{key}çš„éªŒè¯ç»“æœ'

                        all_results.append({
                            'Disease': disease,
                            'å­—æ®µ': field,
                            'æè¿°å†…å®¹': description,
                            'æ˜¯å¦é€šè¿‡': is_passed,
                            'ä¸é€šè¿‡ç†ç”±': reason if is_passed == 'ä¸é€šè¿‡' else ''
                        })

                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSONè§£æå¤±è´¥ - ç–¾ç—…: {disease}, å­—æ®µ: {field}, é”™è¯¯: {str(e)}")
                    logger.error(f"åŸå§‹å“åº”: {response[:500]}...")  # è®°å½•å‰500å­—ç¬¦ç”¨äºè°ƒè¯•

                    # å°è¯•å¤‡ç”¨è§£ææ–¹æ³•
                    backup_results = self.parse_response_backup(response, descriptions)
                    if backup_results:
                        for i, (description, result_data) in enumerate(zip(descriptions, backup_results)):
                            all_results.append({
                                'Disease': disease,
                                'å­—æ®µ': field,
                                'æè¿°å†…å®¹': description,
                                'æ˜¯å¦é€šè¿‡': result_data['result'],
                                'ä¸é€šè¿‡ç†ç”±': result_data['reason'] if result_data['result'] == 'ä¸é€šè¿‡' else ''
                            })
                    else:
                        # è§£æå¤±è´¥ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥ï¼šå…¨éƒ¨é€šè¿‡ä½†æ ‡è®°éœ€è¦äººå·¥å®¡æ ¸
                        for description in descriptions:
                            all_results.append({
                                'Disease': disease,
                                'å­—æ®µ': field,
                                'æè¿°å†…å®¹': description,
                                'æ˜¯å¦é€šè¿‡': 'é€šè¿‡',
                                'ä¸é€šè¿‡ç†ç”±': f'APIå“åº”è§£æå¤±è´¥ï¼Œéœ€è¦äººå·¥å®¡æ ¸: {str(e)}'
                            })
            else:
                # APIè°ƒç”¨å¤±è´¥
                logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥ - ç–¾ç—…: {disease}, å­—æ®µ: {field}")
                for description in descriptions:
                    all_results.append({
                        'Disease': disease,
                        'å­—æ®µ': field,
                        'æè¿°å†…å®¹': description,
                        'æ˜¯å¦é€šè¿‡': 'é€šè¿‡',
                        'ä¸é€šè¿‡ç†ç”±': 'APIè°ƒç”¨å¤±è´¥ï¼Œéœ€è¦äººå·¥å®¡æ ¸'
                    })

            # é¿å…APIè°ƒç”¨è¿‡å¿«
            time.sleep(0.5)

        logger.info(f"âœ… ç–¾ç—… {disease} éªŒè¯å®Œæˆï¼Œå…± {len(all_results)} æ¡ç»“æœ")
        return all_results

    def call_azure_api(self, prompt, max_retries=3):
        """
        è°ƒç”¨Azure OpenAI APIï¼ˆå¸¦ç¼“å­˜ï¼‰
        """
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = hashlib.md5(prompt.encode('utf-8')).hexdigest()
        cache_file = os.path.join(self.cache_dir, "validation_api_responses.json")

        # æ£€æŸ¥ç¼“å­˜
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                if cache_key in cache_data:
                    self.cache_hits += 1
                    return cache_data[cache_key]
            except:
                cache_data = {}
        else:
            cache_data = {}

        # APIè°ƒç”¨
        for attempt in range(max_retries):
            try:
                self.api_calls += 1
                response = self.client.chat.completions.create(
                    model=self.deployment_name,
                    messages=[
                        {"role": "system",
                         "content": "ä½ æ˜¯ä¸€ä½æƒå¨çš„ä¸­åŒ»ä¸“å®¶ï¼Œç²¾é€šä¸­åŒ»ç†è®ºå’Œä¸´åºŠå®è·µï¼Œæ“…é•¿åŸºäºæ ‡å‡†åŒ»å­¦çŸ¥è¯†åº“å®¡æ ¸åŒ»ç–—æè¿°çš„åˆç†æ€§ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                )

                if response.choices and response.choices[0].message and response.choices[0].message.content:
                    result = response.choices[0].message.content.strip()

                    # ä¿å­˜åˆ°ç¼“å­˜
                    cache_data[cache_key] = result
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        json.dump(cache_data, f, ensure_ascii=False, indent=2)

                    return result
                else:
                    logger.warning(f"âš ï¸ APIè¿”å›ç©ºå“åº” (å°è¯• {attempt + 1}/{max_retries})")

            except Exception as e:
                logger.warning(f"âš ï¸ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)

        logger.error("âŒ APIè°ƒç”¨æœ€ç»ˆå¤±è´¥")
        return None

    def parse_medical_data(self, df):
        """
        è§£æçºµå‘å±•å¼€çš„åŒ»ç–—æ•°æ®ï¼ŒæŒ‰ç–¾ç—…åˆ†ç»„
        """
        logger.info("ğŸ“Š å¼€å§‹è§£æåŒ»ç–—æ•°æ®...")

        disease_groups = defaultdict(list)
        current_disease = None

        for index, row in df.iterrows():
            # æ›´æ–°å½“å‰ç–¾ç—…
            if pd.notna(row.get('Disease')):
                current_disease = row['Disease']

            if current_disease is None:
                continue

            # æå–æ¯ä¸ªå­—æ®µçš„æè¿°
            for field in self.key_fields:
                content = row.get(field)
                if pd.notna(content) and str(content).strip():
                    description = str(content).strip()
                    disease_groups[current_disease].append({
                        'field': field,
                        'description': description,
                        'row_index': index
                    })

        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        disease_data_list = []
        for disease, items in disease_groups.items():
            disease_data_list.append({
                'disease': disease,
                'items': items
            })

        total_diseases = len(disease_data_list)
        total_items = sum(len(data['items']) for data in disease_data_list)

        logger.info(f"ğŸ“Š æ•°æ®è§£æå®Œæˆ: {total_diseases} ä¸ªç–¾ç—…, {total_items} æ¡æè¿°")

        # æ˜¾ç¤ºç–¾ç—…åˆ†å¸ƒ
        for data in disease_data_list[:5]:
            logger.info(f"  ğŸ“‹ {data['disease']}: {len(data['items'])} æ¡æè¿°")
        if total_diseases > 5:
            logger.info(f"  ğŸ“‹ ... è¿˜æœ‰ {total_diseases - 5} ä¸ªç–¾ç—…")

        return disease_data_list

    def run_validation(self, disease_data_list):
        """
        è¿è¡ŒéªŒè¯ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        """
        logger.info(f"ğŸš€ å¼€å§‹éªŒè¯ {len(disease_data_list)} ä¸ªç–¾ç—…")

        all_results = []
        for disease_data in tqdm(disease_data_list, desc="éªŒè¯è¿›åº¦", unit="ç–¾ç—…"):
            result = self.validate_single_disease_batch(disease_data)
            all_results.extend(result)

        logger.info(f"âœ… éªŒè¯å®Œæˆï¼Œå…±å¤„ç† {len(all_results)} æ¡æè¿°")
        return all_results

    def save_results(self, results, processing_records, output_dir="../data/case_data"):
        """
        ä¿å­˜éªŒè¯ç»“æœå’Œä¸­é—´å¤„ç†è®°å½•
        """
        logger.info("ğŸ’¾ å¼€å§‹ä¿å­˜ç»“æœ...")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜ä¸»è¦ç»“æœ
        results_file = os.path.join(output_dir, f"åå‘éªŒè¯2kg_{timestamp}.xlsx")
        df_results = pd.DataFrame(results)
        df_results.to_excel(results_file, index=False)
        logger.info(f"ğŸ“„ ä¸»è¦ç»“æœå·²ä¿å­˜: {results_file}")

        # ä¿å­˜ä¸­é—´å¤„ç†è®°å½•
        records_file = os.path.join(output_dir, f"éªŒè¯è¿‡ç¨‹è®°å½•_{timestamp}.xlsx")
        df_records = pd.DataFrame(processing_records)
        df_records.to_excel(records_file, index=False)
        logger.info(f"ğŸ“„ å¤„ç†è®°å½•å·²ä¿å­˜: {records_file}")

        return results_file, records_file

    def generate_statistics(self, results):
        """
        ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            'total': len(results),
            'passed': len([r for r in results if r['æ˜¯å¦é€šè¿‡'] == 'é€šè¿‡']),
            'failed': len([r for r in results if r['æ˜¯å¦é€šè¿‡'] == 'ä¸é€šè¿‡']),
            'diseases_count': len(set(r['Disease'] for r in results)),
        }

        if stats['total'] > 0:
            stats['pass_rate'] = (stats['passed'] / stats['total']) * 100
            stats['fail_rate'] = (stats['failed'] / stats['total']) * 100
        else:
            stats['pass_rate'] = stats['fail_rate'] = 0

        return stats

    def run(self, input_file, max_diseases=None):
        """
        è¿è¡Œå®Œæ•´çš„åŒ»ç–—æè¿°éªŒè¯æµç¨‹
        """
        start_time = time.time()

        logger.info("=" * 100)
        logger.info("ğŸš€ å¼€å§‹åŸºäºçŸ¥è¯†åº“çš„åŒ»ç–—æè¿°åå‘éªŒè¯ä»»åŠ¡")
        if max_diseases:
            logger.info(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ {max_diseases} ä¸ªç–¾ç—…")
        logger.info("=" * 100)

        try:
            # æ­¥éª¤1ï¼šè¯»å–æ•°æ®
            logger.info(f"ğŸ“‹ æ­¥éª¤1: è¯»å–å¾…å®¡æ ¸æ•°æ®æ–‡ä»¶: {input_file}")
            df = pd.read_excel(input_file)
            logger.info(f"âœ… æˆåŠŸè¯»å– {len(df)} è¡Œæ•°æ®")

            # æ­¥éª¤2ï¼šè§£ææ•°æ®
            logger.info("ğŸ“‹ æ­¥éª¤2: è§£æåŒ»ç–—æ•°æ®")
            disease_data_list = self.parse_medical_data(df)

            if not disease_data_list:
                logger.error("âŒ æœªèƒ½è§£æåˆ°ä»»ä½•åŒ»ç–—æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
                return

            # é™åˆ¶å¤„ç†æ•°é‡
            if max_diseases:
                disease_data_list = disease_data_list[:max_diseases]
                logger.info(f"ğŸ”¢ é™åˆ¶å¤„ç†ï¼š{len(disease_data_list)} ä¸ªç–¾ç—…")

            # æ­¥éª¤3ï¼šéªŒè¯æè¿°
            logger.info("ğŸ“‹ æ­¥éª¤3: éªŒè¯æè¿°")
            results = self.run_validation(disease_data_list)

            # æ­¥éª¤4ï¼šä¿å­˜ç»“æœ
            logger.info("ğŸ“‹ æ­¥éª¤4: ä¿å­˜éªŒè¯ç»“æœ")
            results_file, records_file = self.save_results(results, self.processing_records)

            # æ­¥éª¤5ï¼šç”Ÿæˆç»Ÿè®¡
            stats = self.generate_statistics(results)

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            processing_time = end_time - start_time

            logger.info("=" * 100)
            logger.info("ğŸ“Š éªŒè¯å®Œæˆç»Ÿè®¡")
            logger.info("=" * 100)
            logger.info(f"â±ï¸  æ€»å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            logger.info(f"ğŸ”„ APIè°ƒç”¨æ¬¡æ•°: {self.api_calls}")
            logger.info(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­æ¬¡æ•°: {self.cache_hits}")
            if self.api_calls + self.cache_hits > 0:
                cache_rate = self.cache_hits / (self.api_calls + self.cache_hits) * 100
                logger.info(f"ğŸ“ˆ ç¼“å­˜å‘½ä¸­ç‡: {cache_rate:.1f}%")

            logger.info(f"ğŸ¥ å¤„ç†ç–¾ç—…æ•°: {stats['diseases_count']}")
            logger.info(f"ğŸ“ éªŒè¯æè¿°æ€»æ•°: {stats['total']}")
            logger.info(f"âœ… é€šè¿‡: {stats['passed']} ({stats['pass_rate']:.1f}%)")
            logger.info(f"âŒ ä¸é€šè¿‡: {stats['failed']} ({stats['fail_rate']:.1f}%)")

            logger.info(f"ğŸ“„ ä¸»è¦ç»“æœæ–‡ä»¶: {results_file}")
            logger.info(f"ğŸ“„ è¿‡ç¨‹è®°å½•æ–‡ä»¶: {records_file}")
            logger.info("")
            logger.info("ğŸ“‹ è¾“å‡ºæ ¼å¼è¯´æ˜:")
            logger.info("  - ä¸»è¦ç»“æœ: Disease | å­—æ®µ | æè¿°å†…å®¹ | æ˜¯å¦é€šè¿‡ | ä¸é€šè¿‡ç†ç”±")
            logger.info("  - è¿‡ç¨‹è®°å½•: åŒ…å«ç–¾ç—…åŒ¹é…ã€çŸ¥è¯†åº“ä½¿ç”¨ã€å®Œæ•´promptç­‰è¯¦ç»†ä¿¡æ¯")
            logger.info("")
            logger.info("ğŸ” é‡ç‚¹å…³æ³¨:")
            logger.info("  - æ ‡è®°ä¸º'ä¸é€šè¿‡'çš„æè¿°éœ€è¦åŒ»ç”Ÿé‡æ–°å®¡æ ¸")
            logger.info("  - æŸ¥çœ‹è¿‡ç¨‹è®°å½•æ–‡ä»¶äº†è§£å…·ä½“çš„çŸ¥è¯†åº“åŒ¹é…å’Œä½¿ç”¨æƒ…å†µ")
            logger.info("=" * 100)

        except Exception as e:
            logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
            raise


def main():
    """
    ä¸»å‡½æ•°ï¼Œé…ç½®å‚æ•°å¹¶è¿è¡ŒåŸºäºçŸ¥è¯†åº“çš„åŒ»ç–—æè¿°éªŒè¯
    """
    # Azure OpenAIé…ç½®
    AZURE_API_KEY = ""  # è¯·å¡«å…¥æ‚¨çš„APIå¯†é’¥
    AZURE_ENDPOINT = ""  # è¯·å¡«å…¥æ‚¨çš„ç«¯ç‚¹åœ°å€
    DEPLOYMENT_NAME = "o3"  # æ‚¨çš„éƒ¨ç½²åç§°

    # æ–‡ä»¶è·¯å¾„é…ç½®
    input_file = "../../data/case_data/ç—…å†è¡¨_å¾…å®¡æ ¸_20250810.xlsx"
    knowledge_base_path = "../../data/other/book_structured_enhanced.json"

    # æµ‹è¯•é…ç½®ï¼ˆè®¾ç½®ä¸ºNoneå¤„ç†å…¨éƒ¨ç–¾ç—…ï¼Œè®¾ç½®æ•°å­—åªå¤„ç†å‰Nä¸ªç–¾ç—…ï¼‰
    MAX_DISEASES = None  # å»ºè®®å…ˆæµ‹è¯•5ä¸ªç–¾ç—…ï¼Œæ•ˆæœæ»¡æ„åæ”¹ä¸ºNoneå¤„ç†å…¨éƒ¨ç–¾ç—…

    # æ£€æŸ¥é…ç½®
    if not AZURE_API_KEY or AZURE_API_KEY == "":
        logger.error("âŒ è¯·å…ˆé…ç½®Azure OpenAI APIå¯†é’¥ï¼")
        logger.info("è¯·ä¿®æ”¹main()å‡½æ•°ä¸­çš„AZURE_API_KEYå‚æ•°")
        return

    if not AZURE_ENDPOINT or AZURE_ENDPOINT == "":
        logger.error("âŒ è¯·å…ˆé…ç½®Azure OpenAIç«¯ç‚¹åœ°å€ï¼")
        logger.info("è¯·ä¿®æ”¹main()å‡½æ•°ä¸­çš„AZURE_ENDPOINTå‚æ•°")
        return

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return

    if not os.path.exists(knowledge_base_path):
        logger.error(f"âŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {knowledge_base_path}")
        return

    # åˆ›å»ºéªŒè¯å™¨
    validator = KnowledgeBasedValidator(
        azure_api_key=AZURE_API_KEY,
        azure_endpoint=AZURE_ENDPOINT,
        deployment_name=DEPLOYMENT_NAME,
        knowledge_base_path=knowledge_base_path
    )

    # è¿è¡ŒéªŒè¯æµç¨‹
    try:
        validator.run(input_file, max_diseases=MAX_DISEASES)
    except KeyboardInterrupt:
        logger.info("âš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¼‚å¸¸: {str(e)}")


if __name__ == "__main__":
    main()
