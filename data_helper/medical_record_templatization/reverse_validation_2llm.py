import pandas as pd
import json
import logging
from datetime import datetime
from collections import defaultdict
from openai import AzureOpenAI
import time
import os
import hashlib
"""
åŒ»ç”Ÿç—…å†å®¡æ ¸ç»“æ„åå‘éªŒè¯ï¼ŒåŸºäºæ¨¡å‹çŸ¥è¯†å‡ºäº†ä¸€ç‰ˆç»“æœ
â— è¾“å‡ºæ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ª"ç–¾ç—…+å­—æ®µ+æè¿°+å®¡æ ¸ç»“æœ+ç½®ä¿¡åº¦"
â— âœ… å®¡æ ¸ç»“æœï¼šé€šè¿‡/ä¸é€šè¿‡/å¾…å®šï¼ˆAIä¸ç¡®å®šæ—¶å†™å¾…å®šï¼‰
â— âœ… è¾“å‡ºåˆ°ï¼š../data/case_data/åå‘éªŒè¯0810_all.xlsx
â— âœ… æ–°æ–‡ä»¶ï¼Œä¸ä¿®æ”¹åŸè¡¨

ç»“æœï¼š
å¤„ç†ç–¾ç—…æ•°: 176
éªŒè¯æè¿°æ€»æ•°: 3062
é€šè¿‡ï¼š2639ï¼ˆ86.2%ï¼‰
ä¸é€šè¿‡ï¼š169ï¼ˆ5.5%ï¼‰
å¾…å®šï¼š254ï¼ˆ8.3%ï¼‰
"""
# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MedicalDescriptionValidator:
    """
    åŒ»ç–—æè¿°åˆç†æ€§å®¡æ ¸å·¥å…·
    éªŒè¯ç–¾ç—…ä¸ç—‡çŠ¶æè¿°çš„åŒ»å­¦åˆç†æ€§
    """

    def __init__(self, azure_api_key, azure_endpoint, deployment_name="o3", cache_dir="cache"):
        """
        åˆå§‹åŒ–å®¡æ ¸å·¥å…·

        Args:
            azure_api_key (str): Azure OpenAI APIå¯†é’¥
            azure_endpoint (str): Azure OpenAIç«¯ç‚¹
            deployment_name (str): éƒ¨ç½²åç§°ï¼Œé»˜è®¤o3
            cache_dir (str): ç¼“å­˜ç›®å½•
        """
        # ä¿®å¤endpointæ ¼å¼
        if azure_endpoint.endswith('chat/completions?'):
            azure_endpoint = azure_endpoint.replace('/openai/deployments/o3/chat/completions?', '')

        self.client = AzureOpenAI(
            api_key=azure_api_key,
            api_version="2025-01-01-preview",
            azure_endpoint=azure_endpoint
        )
        self.deployment_name = deployment_name

        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

        # å®šä¹‰8ä¸ªå…³é”®åŒ»ç–—å­—æ®µ
        self.key_fields = ['ä¸»è¯‰', 'ç°ç—…å²', 'æ—¢å¾€å²', 'è¾…åŠ©æ£€æŸ¥', 'PE/æ£€æŸ¥ ï¼ˆä½“ç°æœ›é—»é—®åˆ‡ï¼‰', 'ç—…æœº', 'æ²»åˆ™/å¤„ç†',
                           'åŒ»å˜±']

        # APIè°ƒç”¨ç»Ÿè®¡
        self.api_calls = 0
        self.cache_hits = 0

    def get_cache_key(self, text):
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def load_cache(self, cache_file):
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        cache_path = os.path.join(self.cache_dir, cache_file)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_cache(self, cache_file, cache_data):
        """ä¿å­˜ç¼“å­˜æ–‡ä»¶"""
        cache_path = os.path.join(self.cache_dir, cache_file)
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

    def call_azure_api(self, prompt, max_retries=3):
        """
        è°ƒç”¨Azure OpenAI APIï¼ˆå¸¦ç¼“å­˜ï¼‰
        """
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self.get_cache_key(prompt)
        cache_file = "validation_api_responses.json"
        cache_data = self.load_cache(cache_file)

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in cache_data:
            self.cache_hits += 1
            return cache_data[cache_key]

        # APIè°ƒç”¨
        for attempt in range(max_retries):
            try:
                self.api_calls += 1
                response = self.client.chat.completions.create(
                    model=self.deployment_name,
                    messages=[
                        {"role": "system",
                         "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä¸­è¥¿åŒ»ç»“åˆä¸“å®¶ï¼Œç²¾é€šä¸­åŒ»ç†è®ºå’Œç°ä»£åŒ»å­¦ï¼Œæ“…é•¿åˆ¤æ–­ç–¾ç—…ä¸ç—‡çŠ¶æè¿°çš„åŒ»å­¦åˆç†æ€§ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                )

                if response.choices and response.choices[0].message and response.choices[0].message.content:
                    result = response.choices[0].message.content.strip()
                    # ä¿å­˜åˆ°ç¼“å­˜
                    cache_data[cache_key] = result
                    self.save_cache(cache_file, cache_data)
                    return result
                else:
                    logger.warning(f"APIè¿”å›ç©ºå“åº” (å°è¯• {attempt + 1}/{max_retries})")

            except Exception as e:
                logger.warning(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)

        logger.error("APIè°ƒç”¨æœ€ç»ˆå¤±è´¥")
        return None

    def parse_medical_data(self, df):
        """
        è§£æçºµå‘å±•å¼€çš„åŒ»ç–—æ•°æ®

        Args:
            df (pd.DataFrame): åŸå§‹æ•°æ®

        Returns:
            list: [(disease, field, description), ...] æ‰€æœ‰éœ€è¦éªŒè¯çš„æ¡ç›®
        """
        logger.info("è§£æçºµå‘å±•å¼€çš„åŒ»ç–—æ•°æ®...")

        validation_items = []
        current_disease = None

        for index, row in df.iterrows():
            # æ›´æ–°å½“å‰ç–¾ç—…
            if pd.notna(row.get('Disease')):
                current_disease = row['Disease']

            if current_disease is None:
                continue

            # æå–æ¯ä¸ªå­—æ®µçš„æè¿°
            for field in self.key_fields:
                content = row.get(field)
                if pd.notna(content) and str(content).strip():
                    description = str(content).strip()
                    validation_items.append((current_disease, field, description))

        logger.info(f"è§£æå®Œæˆ: {len(validation_items)} æ¡æè¿°éœ€è¦éªŒè¯")

        # ç»Ÿè®¡ä¿¡æ¯
        disease_count = len(set(item[0] for item in validation_items))
        logger.info(f"æ¶‰åŠ {disease_count} ä¸ªç–¾ç—…")

        return validation_items

    def batch_validate_descriptions(self, validation_items):
        """
        æ‰¹é‡éªŒè¯æè¿°

        Args:
            validation_items (list): [(disease, field, description), ...] å¾…éªŒè¯æ¡ç›®

        Returns:
            list: [{'Disease': ..., 'å­—æ®µ': ..., 'æè¿°å†…å®¹': ..., 'å®¡æ ¸ç»“æœ': ..., 'ç½®ä¿¡åº¦': ...}, ...]
        """
        logger.info("å¼€å§‹æ‰¹é‡éªŒè¯æè¿°...")

        results = []
        batch_size = 10  # æ¯æ‰¹å¤„ç†10æ¡
        total_batches = (len(validation_items) + batch_size - 1) // batch_size

        for batch_idx in range(0, len(validation_items), batch_size):
            batch_items = validation_items[batch_idx:batch_idx + batch_size]
            current_batch = batch_idx // batch_size + 1

            logger.info(f"å¤„ç†æ‰¹æ¬¡ {current_batch}/{total_batches} ({len(batch_items)} æ¡)")

            # æ„å»ºæ‰¹é‡éªŒè¯prompt
            items_text = []
            for i, (disease, field, description) in enumerate(batch_items, 1):
                items_text.append(f"{i}. ç–¾ç—…ï¼š{disease} | å­—æ®µï¼š{field} | æè¿°ï¼š{description}")

            prompt = f"""
è¯·ä½œä¸ºä¸­è¥¿åŒ»ç»“åˆä¸“å®¶ï¼Œæ‰¹é‡éªŒè¯ä»¥ä¸‹{len(batch_items)}æ¡åŒ»ç–—æè¿°çš„åˆç†æ€§ï¼š

éªŒè¯åˆ—è¡¨ï¼š
{chr(10).join(items_text)}

éªŒè¯æ ‡å‡†ï¼ˆé‡‡ç”¨ä¸´åºŠå®é™…æ ‡å‡†ï¼Œä¸è¦è¿‡äºä¸¥æ ¼ï¼‰ï¼š
1. **ä¸´åºŠåˆç†æ€§**ï¼šè¯¥æè¿°æ˜¯å¦å¯èƒ½å‡ºç°åœ¨è¯¥ç–¾ç—…æ‚£è€…èº«ä¸Šï¼Ÿï¼ˆåŒ…æ‹¬ç›´æ¥ç—‡çŠ¶ã€å¹¶å‘ç—‡çŠ¶ã€ä»£å¿ç—‡çŠ¶ã€ä¼´éšç—‡çŠ¶ç­‰ï¼‰
2. **åŒ»å­¦å…³è”æ€§**ï¼šè¯¥æè¿°ä¸ç–¾ç—…æ˜¯å¦æœ‰ç›´æ¥æˆ–é—´æ¥çš„åŒ»å­¦å…³è”ï¼Ÿ
3. **å­—æ®µé€‚é…æ€§**ï¼šè¯¥æè¿°æ˜¯å¦é€‚åˆåœ¨è¯¥å­—æ®µä¸­è®°å½•ï¼Ÿ

é‡è¦åŸåˆ™ï¼š
- **å®½æ¾åˆ¤æ–­**ï¼šåªè¦æè¿°ä¸ç–¾ç—…æœ‰åˆç†çš„åŒ»å­¦å…³è”å°±åº”è¯¥é€šè¿‡
- **ä¸´åºŠè§†è§’**ï¼šä»å®é™…ä¸´åºŠå·¥ä½œè§’åº¦è€ƒè™‘ï¼Œæ‚£è€…å¯èƒ½çš„å¤æ‚è¡¨ç°
- **ä¸­åŒ»ç‰¹è‰²**ï¼šä¸­åŒ»å¼ºè°ƒæ•´ä½“è§‚å¿µï¼Œç—‡çŠ¶å¯èƒ½æ¶‰åŠå¤šç³»ç»Ÿ
- **ä»£å¿æœºåˆ¶**ï¼šè€ƒè™‘ç–¾ç—…å¼•èµ·çš„ä»£å¿æ€§æ”¹å˜å’Œç»§å‘ç—‡çŠ¶

å…·ä½“åˆ¤æ–­è¦ç‚¹ï¼š
- é¢ˆæ¤ç—…ï¼šå¯èƒ½å¼•èµ·é¢ˆè‚©è…°èƒŒç–¼ç—›ã€å¤´ç—›å¤´æ™•ã€ä¸Šè‚¢éº»æœ¨ç­‰ï¼Œå› ä¸ºè„ŠæŸ±æ˜¯ä¸€ä¸ªæ•´ä½“
- è…°è‚ŒåŠ³æŸï¼šå¯èƒ½ä¼´å‘é¢ˆè‚©é—®é¢˜ï¼Œå› ä¸ºå§¿åŠ¿ä»£å¿
- ç—…æœºï¼šä¸­åŒ»ç—…æœºå¯ä»¥å¤šæ ·åŒ–ï¼Œä¸è¦æ±‚å•ä¸€æ ‡å‡†ç­”æ¡ˆ
- æ²»æ³•ï¼šåªè¦ä¸æ˜¯æ˜æ˜¾é”™è¯¯çš„æ²»æ³•éƒ½å¯ä»¥é€šè¿‡
- ç—‡çŠ¶ï¼šè€ƒè™‘ç–¾ç—…çš„å¤šæ ·æ€§è¡¨ç°å’Œä¸ªä½“å·®å¼‚

å®¡æ ¸ç»“æœåˆ¤æ–­ï¼š
- **é€šè¿‡**ï¼šæè¿°åœ¨ä¸´åºŠä¸Šæ˜¯åˆç†çš„ã€å¯èƒ½å‡ºç°çš„
- **ä¸é€šè¿‡**ï¼šæè¿°æ˜æ˜¾åŒ»å­¦é”™è¯¯ï¼Œå®Œå…¨ä¸å¯èƒ½ä¸è¯¥ç–¾ç—…ç›¸å…³
- **å¾…å®š**ï¼šæè¿°å­˜åœ¨ä¸€å®šäº‰è®®ï¼Œéœ€è¦æ›´å¤šä¸´åºŠä¿¡æ¯åˆ¤æ–­

ç½®ä¿¡åº¦åˆ¤æ–­ï¼š
- **é«˜**ï¼šåˆ¤æ–­éå¸¸ç¡®å®šï¼Œæœ‰å……åˆ†åŒ»å­¦ä¾æ®
- **ä¸­**ï¼šåˆ¤æ–­è¾ƒç¡®å®šï¼ŒåŸºäºä¸´åºŠç»éªŒå’ŒåŒ»å­¦å¸¸è¯†
- **ä½**ï¼šåˆ¤æ–­ä¸å¤Ÿç¡®å®šï¼Œå¯èƒ½å­˜åœ¨ä¾‹å¤–æƒ…å†µ

è¯·è¿”å›JSONæ ¼å¼ï¼Œé”®ä¸ºæ¡ç›®ç¼–å·(1,2,3...)ï¼Œå€¼ä¸ºéªŒè¯ç»“æœï¼š
{{
  "1": {{
    "result": "é€šè¿‡/ä¸é€šè¿‡/å¾…å®š",
    "confidence": "é«˜/ä¸­/ä½"
  }},
  "2": {{
    "result": "é€šè¿‡/ä¸é€šè¿‡/å¾…å®š", 
    "confidence": "é«˜/ä¸­/ä½"
  }}
}}

ç¤ºä¾‹ï¼š
{{"1": {{"result": "é€šè¿‡", "confidence": "é«˜"}}, "2": {{"result": "ä¸é€šè¿‡", "confidence": "é«˜"}}, "3": {{"result": "å¾…å®š", "confidence": "ä¸­"}}}}
"""

            response = self.call_azure_api(prompt)

            if response:
                try:
                    validation_data = json.loads(response)

                    for i, (disease, field, description) in enumerate(batch_items, 1):
                        key = str(i)
                        if key in validation_data and isinstance(validation_data[key], dict):
                            result_data = validation_data[key]
                            audit_result = result_data.get('result', 'å¾…å®š')
                            confidence = result_data.get('confidence', 'ä½')
                        else:
                            audit_result = 'å¾…å®š'
                            confidence = 'ä½'

                        results.append({
                            'Disease': disease,
                            'å­—æ®µ': field,
                            'æè¿°å†…å®¹': description,
                            'å®¡æ ¸ç»“æœ': audit_result,
                            'ç½®ä¿¡åº¦': confidence
                        })

                except json.JSONDecodeError:
                    logger.warning(f"æ‰¹æ¬¡ {current_batch} JSONè§£æå¤±è´¥ï¼Œæ ‡è®°ä¸ºå¾…å®š")
                    # è§£æå¤±è´¥ï¼Œå…¨éƒ¨æ ‡è®°ä¸ºå¾…å®š
                    for disease, field, description in batch_items:
                        results.append({
                            'Disease': disease,
                            'å­—æ®µ': field,
                            'æè¿°å†…å®¹': description,
                            'å®¡æ ¸ç»“æœ': 'å¾…å®š',
                            'ç½®ä¿¡åº¦': 'ä½'
                        })
            else:
                # APIè°ƒç”¨å¤±è´¥ï¼Œå…¨éƒ¨æ ‡è®°ä¸ºå¾…å®š
                logger.warning(f"æ‰¹æ¬¡ {current_batch} APIè°ƒç”¨å¤±è´¥ï¼Œæ ‡è®°ä¸ºå¾…å®š")
                for disease, field, description in batch_items:
                    results.append({
                        'Disease': disease,
                        'å­—æ®µ': field,
                        'æè¿°å†…å®¹': description,
                        'å®¡æ ¸ç»“æœ': 'å¾…å®š',
                        'ç½®ä¿¡åº¦': 'ä½'
                    })

            # é¿å…APIè°ƒç”¨è¿‡å¿«
            time.sleep(0.5)

        return results

    def generate_statistics(self, results):
        """
        ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯

        Args:
            results (list): éªŒè¯ç»“æœåˆ—è¡¨

        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            'total': len(results),
            'passed': len([r for r in results if r['å®¡æ ¸ç»“æœ'] == 'é€šè¿‡']),
            'failed': len([r for r in results if r['å®¡æ ¸ç»“æœ'] == 'ä¸é€šè¿‡']),
            'pending': len([r for r in results if r['å®¡æ ¸ç»“æœ'] == 'å¾…å®š']),
            'high_confidence': len([r for r in results if r['ç½®ä¿¡åº¦'] == 'é«˜']),
            'medium_confidence': len([r for r in results if r['ç½®ä¿¡åº¦'] == 'ä¸­']),
            'low_confidence': len([r for r in results if r['ç½®ä¿¡åº¦'] == 'ä½']),
            'diseases_count': len(set(r['Disease'] for r in results)),
        }

        if stats['total'] > 0:
            stats['pass_rate'] = (stats['passed'] / stats['total']) * 100
            stats['fail_rate'] = (stats['failed'] / stats['total']) * 100
            stats['pending_rate'] = (stats['pending'] / stats['total']) * 100
        else:
            stats['pass_rate'] = stats['fail_rate'] = stats['pending_rate'] = 0

        return stats

    def run(self, input_file, max_diseases=None):
        """
        è¿è¡Œå®Œæ•´çš„åŒ»ç–—æè¿°éªŒè¯æµç¨‹

        Args:
            input_file (str): è¾“å…¥Excelæ–‡ä»¶è·¯å¾„
            max_diseases (int): æœ€å¤§å¤„ç†ç–¾ç—…æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†å…¨éƒ¨
        """
        start_time = time.time()

        logger.info("=" * 80)
        logger.info("å¼€å§‹åŒ»ç–—æè¿°åå‘éªŒè¯ä»»åŠ¡")
        if max_diseases:
            logger.info(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ {max_diseases} ä¸ªç–¾ç—…")
        logger.info("=" * 80)

        try:
            # æ­¥éª¤1ï¼šè¯»å–æ•°æ®
            logger.info(f"ğŸ“‹ æ­¥éª¤1: è¯»å–å¾…å®¡æ ¸æ•°æ®æ–‡ä»¶: {input_file}")
            df = pd.read_excel(input_file)
            logger.info(f"æˆåŠŸè¯»å– {len(df)} è¡Œæ•°æ®")

            # æ­¥éª¤2ï¼šè§£æçºµå‘æ•°æ®
            logger.info("ğŸ“‹ æ­¥éª¤2: è§£æåŒ»ç–—æ•°æ®")
            validation_items = self.parse_medical_data(df)

            if not validation_items:
                logger.error("æœªèƒ½è§£æåˆ°ä»»ä½•åŒ»ç–—æ•°æ®ï¼Œç¨‹åºç»ˆæ­¢")
                return

            # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆå¦‚æœè®¾ç½®äº†max_diseasesï¼‰
            if max_diseases:
                # æŒ‰ç–¾ç—…åˆ†ç»„
                disease_items = defaultdict(list)
                for item in validation_items:
                    disease_items[item[0]].append(item)

                # åªå–å‰Nä¸ªç–¾ç—…
                limited_diseases = list(disease_items.keys())[:max_diseases]
                validation_items = []
                for disease in limited_diseases:
                    validation_items.extend(disease_items[disease])

                logger.info(f"é™åˆ¶å¤„ç†ï¼š{len(limited_diseases)} ä¸ªç–¾ç—…ï¼Œ{len(validation_items)} æ¡æè¿°")

            # æ­¥éª¤3ï¼šæ‰¹é‡éªŒè¯æè¿°
            logger.info("ğŸ“‹ æ­¥éª¤3: æ‰¹é‡éªŒè¯æè¿°åˆç†æ€§")
            results = self.batch_validate_descriptions(validation_items)

            # æ­¥éª¤4ï¼šä¿å­˜ç»“æœ
            logger.info("ğŸ“‹ æ­¥éª¤4: ä¿å­˜éªŒè¯ç»“æœ")

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = "../../data/case_data"
            os.makedirs(output_dir, exist_ok=True)

            output_file = os.path.join(output_dir, "åå‘éªŒè¯0810_all.xlsx")

            # ä¿å­˜åˆ°Excel
            df_results = pd.DataFrame(results)
            df_results.to_excel(output_file, index=False)

            # æ­¥éª¤5ï¼šç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            stats = self.generate_statistics(results)

            # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            processing_time = end_time - start_time

            logger.info("=" * 80)
            logger.info("ğŸ“Š åå‘éªŒè¯å®Œæˆç»Ÿè®¡")
            logger.info("=" * 80)
            logger.info(f"å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            logger.info(f"APIè°ƒç”¨æ¬¡æ•°: {self.api_calls}")
            logger.info(f"ç¼“å­˜å‘½ä¸­æ¬¡æ•°: {self.cache_hits}")
            if self.api_calls + self.cache_hits > 0:
                cache_rate = self.cache_hits / (self.api_calls + self.cache_hits) * 100
                logger.info(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_rate:.1f}%")

            logger.info(f"å¤„ç†ç–¾ç—…æ•°: {stats['diseases_count']}")
            logger.info(f"éªŒè¯æè¿°æ€»æ•°: {stats['total']}")
            logger.info(f"é€šè¿‡: {stats['passed']} ({stats['pass_rate']:.1f}%)")
            logger.info(f"ä¸é€šè¿‡: {stats['failed']} ({stats['fail_rate']:.1f}%)")
            logger.info(f"å¾…å®š: {stats['pending']} ({stats['pending_rate']:.1f}%)")
            logger.info("")
            logger.info(f"é«˜ç½®ä¿¡åº¦: {stats['high_confidence']}")
            logger.info(f"ä¸­ç½®ä¿¡åº¦: {stats['medium_confidence']}")
            logger.info(f"ä½ç½®ä¿¡åº¦: {stats['low_confidence']}")
            logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
            logger.info("")
            logger.info("ğŸ“‹ è¾“å‡ºæ ¼å¼: Disease | å­—æ®µ | æè¿°å†…å®¹ | å®¡æ ¸ç»“æœ | ç½®ä¿¡åº¦")
            logger.info("ğŸ” é‡ç‚¹å…³æ³¨: æ ‡è®°ä¸º'ä¸é€šè¿‡'å’Œ'å¾…å®š'çš„æè¿°")
            logger.info("=" * 80)

        except Exception as e:
            logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
            raise


def main():
    """
    ä¸»å‡½æ•°ï¼Œé…ç½®å‚æ•°å¹¶è¿è¡ŒåŒ»ç–—æè¿°éªŒè¯
    """
    # Azure OpenAIé…ç½®
    AZURE_API_KEY = ""
    AZURE_ENDPOINT = ""
    DEPLOYMENT_NAME = "o3"  # æ‚¨çš„éƒ¨ç½²åç§°

    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    input_file = "../../data/case_data/ç—…å†è¡¨_å¾…å®¡æ ¸_20250808_.xlsx"

    # æµ‹è¯•é…ç½®ï¼ˆè®¾ç½®ä¸ºNoneå¤„ç†å…¨éƒ¨ç–¾ç—…ï¼Œè®¾ç½®æ•°å­—åªå¤„ç†å‰Nä¸ªç–¾ç—…ï¼‰
    MAX_DISEASES = None  # å»ºè®®å…ˆæµ‹è¯•5ä¸ªç–¾ç—…ï¼Œæ•ˆæœæ»¡æ„åæ”¹ä¸ºNoneå¤„ç†å…¨éƒ¨176ä¸ªç–¾ç—…

    # æ£€æŸ¥é…ç½®
    if not AZURE_API_KEY or AZURE_API_KEY == "":
        logger.error("è¯·å…ˆé…ç½®Azure OpenAI APIå¯†é’¥ï¼")
        logger.info("è¯·ä¿®æ”¹main()å‡½æ•°ä¸­çš„AZURE_API_KEYå‚æ•°")
        return

    if not AZURE_ENDPOINT or AZURE_ENDPOINT == "":
        logger.error("è¯·å…ˆé…ç½®Azure OpenAIç«¯ç‚¹åœ°å€ï¼")
        logger.info("è¯·ä¿®æ”¹main()å‡½æ•°ä¸­çš„AZURE_ENDPOINTå‚æ•°")
        return

    # åˆ›å»ºéªŒè¯å™¨å¹¶è¿è¡Œ
    validator = MedicalDescriptionValidator(
        azure_api_key=AZURE_API_KEY,
        azure_endpoint=AZURE_ENDPOINT,
        deployment_name=DEPLOYMENT_NAME
    )

    # è¿è¡ŒéªŒè¯æµç¨‹
    validator.run(input_file, max_diseases=MAX_DISEASES)


if __name__ == "__main__":
    main()
