import pandas as pd
import json
import logging
from datetime import datetime
import time
import os
import hashlib
from collections import defaultdict
from openai import AzureOpenAI
import difflib
from fuzzywuzzy import fuzz
import numpy as np

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DiseaseMatchingSystem:
    """
    ä¸­åŒ»ç–¾ç—…åŒ¹é…éªŒè¯ç³»ç»Ÿ
    """

    def __init__(self, azure_api_key=None, azure_endpoint=None, deployment_name="o3", cache_dir="cache"):
        """
        åˆå§‹åŒ–åŒ¹é…ç³»ç»Ÿ

        Args:
            azure_api_key (str): Azure OpenAI APIå¯†é’¥ (ç”¨äºè¯­ä¹‰åŒ¹é…)
            azure_endpoint (str): Azure OpenAIç«¯ç‚¹
            deployment_name (str): éƒ¨ç½²åç§°
            cache_dir (str): ç¼“å­˜ç›®å½•
        """
        self.similarity_threshold = 0.7
        self.llm_threshold_low = 0.4
        self.llm_threshold_high = 0.7

        # Azure OpenAIé…ç½® (å¯é€‰)
        self.use_llm = azure_api_key and azure_endpoint
        if self.use_llm:
            # ä¿®å¤endpointæ ¼å¼
            if azure_endpoint.endswith('chat/completions?'):
                azure_endpoint = azure_endpoint.replace('/openai/deployments/o3/chat/completions?', '')

            self.client = AzureOpenAI(
                api_key=azure_api_key,
                api_version="2025-01-01-preview",
                azure_endpoint=azure_endpoint
            )
            self.deployment_name = deployment_name

            # åˆ›å»ºç¼“å­˜ç›®å½•
            self.cache_dir = cache_dir
            os.makedirs(cache_dir, exist_ok=True)

            # APIè°ƒç”¨ç»Ÿè®¡
            self.api_calls = 0
            self.cache_hits = 0
        else:
            logger.info("æœªé…ç½®Azure OpenAIï¼Œå°†è·³è¿‡è¯­ä¹‰åŒ¹é…æ­¥éª¤")

    def load_data(self, diseases_txt_path, json_path):
        """
        åŠ è½½æ•°æ®æ–‡ä»¶

        Args:
            diseases_txt_path (str): é—¨è¯Šç–¾ç—…åtxtæ–‡ä»¶è·¯å¾„
            json_path (str): æ ‡å‡†ç–¾ç—…JSONæ–‡ä»¶è·¯å¾„

        Returns:
            tuple: (é—¨è¯Šç–¾ç—…åˆ—è¡¨, æ ‡å‡†ç–¾ç—…åˆ—è¡¨)
        """
        logger.info("å¼€å§‹åŠ è½½æ•°æ®æ–‡ä»¶...")

        # è¯»å–é—¨è¯Šç–¾ç—…å
        with open(diseases_txt_path, 'r', encoding='utf-8') as f:
            clinic_diseases = [line.strip() for line in f if line.strip()]

        # è¯»å–æ ‡å‡†ç–¾ç—…JSON
        with open(json_path, 'r', encoding='utf-8') as f:
            standard_data = json.load(f)
            standard_diseases = list(standard_data.keys())

        logger.info(f"é—¨è¯Šç–¾ç—…æ•°é‡: {len(clinic_diseases)}")
        logger.info(f"æ ‡å‡†ç–¾ç—…æ•°é‡: {len(standard_diseases)}")

        return clinic_diseases, standard_diseases

    def exact_match(self, clinic_disease, standard_diseases):
        """
        ç²¾ç¡®åŒ¹é…

        Args:
            clinic_disease (str): é—¨è¯Šç–¾ç—…å
            standard_diseases (list): æ ‡å‡†ç–¾ç—…åˆ—è¡¨

        Returns:
            tuple: (æ˜¯å¦åŒ¹é…, åŒ¹é…çš„ç–¾ç—…å)
        """
        if clinic_disease in standard_diseases:
            return True, clinic_disease
        return False, None

    def levenshtein_distance(self, s1, s2):
        """è®¡ç®—ç¼–è¾‘è·ç¦»"""
        if len(s1) > len(s2):
            s1, s2 = s2, s1

        distances = range(len(s1) + 1)
        for i2, c2 in enumerate(s2):
            distances_ = [i2 + 1]
            for i1, c1 in enumerate(s1):
                if c1 == c2:
                    distances_.append(distances[i1])
                else:
                    distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))
            distances = distances_

        # è¿”å›ç›¸ä¼¼åº¦ (0-1)
        max_len = max(len(s1), len(s2))
        if max_len == 0:
            return 1.0
        return 1.0 - distances[-1] / max_len

    def jaccard_similarity(self, s1, s2, n=2):
        """è®¡ç®—Jaccardç›¸ä¼¼åº¦ (åŸºäºn-gram)"""

        def get_ngrams(text, n):
            return set([text[i:i + n] for i in range(len(text) - n + 1)])

        ngrams1 = get_ngrams(s1, n)
        ngrams2 = get_ngrams(s2, n)

        if not ngrams1 and not ngrams2:
            return 1.0
        if not ngrams1 or not ngrams2:
            return 0.0

        intersection = len(ngrams1.intersection(ngrams2))
        union = len(ngrams1.union(ngrams2))

        return intersection / union

    def cosine_similarity(self, s1, s2):
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (åŸºäºå­—ç¬¦å‘é‡)"""
        # åˆ›å»ºå­—ç¬¦é›†åˆ
        chars = set(s1 + s2)
        if not chars:
            return 1.0

        # åˆ›å»ºå‘é‡
        vec1 = [s1.count(c) for c in chars]
        vec2 = [s2.count(c) for c in chars]

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5

        if norm1 == 0 or norm2 == 0:
            return 0.0
        return dot_product / (norm1 * norm2)

    def lcs_similarity(self, s1, s2):
        """æœ€é•¿å…¬å…±å­åºåˆ—ç›¸ä¼¼åº¦"""
        m, n = len(s1), len(s2)
        if m == 0 or n == 0:
            return 0.0

        # åŠ¨æ€è§„åˆ’è®¡ç®—LCSé•¿åº¦
        dp = [[0] * (n + 1) for _ in range(m + 1)]

        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if s1[i - 1] == s2[j - 1]:
                    dp[i][j] = dp[i - 1][j - 1] + 1
                else:
                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])

        lcs_length = dp[m][n]
        max_length = max(m, n)

        return lcs_length / max_length

    def jaro_winkler_similarity(self, s1, s2):
        """Jaro-Winklerç›¸ä¼¼åº¦ (ç®€åŒ–å®ç°)"""
        # ä½¿ç”¨difflibä½œä¸ºè¿‘ä¼¼å®ç°
        return difflib.SequenceMatcher(None, s1, s2).ratio()

    def calculate_similarity(self, clinic_disease, standard_disease):
        """
        è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦

        Args:
            clinic_disease (str): é—¨è¯Šç–¾ç—…å
            standard_disease (str): æ ‡å‡†ç–¾ç—…å

        Returns:
            float: ç›¸ä¼¼åº¦ (0-1)
        """
        # å¤šç§ç®—æ³•è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []

        # 1. ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦
        similarities.append(self.levenshtein_distance(clinic_disease, standard_disease))

        # 2. Jaccardç›¸ä¼¼åº¦
        similarities.append(self.jaccard_similarity(clinic_disease, standard_disease))

        # 3. ä½™å¼¦ç›¸ä¼¼åº¦
        similarities.append(self.cosine_similarity(clinic_disease, standard_disease))

        # 4. LCSç›¸ä¼¼åº¦
        similarities.append(self.lcs_similarity(clinic_disease, standard_disease))

        # 5. Jaro-Winklerç›¸ä¼¼åº¦
        similarities.append(self.jaro_winkler_similarity(clinic_disease, standard_disease))

        # 6. fuzzywuzzy ratio
        similarities.append(fuzz.ratio(clinic_disease, standard_disease) / 100.0)

        # å–æœ€å¤§å€¼ä½œä¸ºæœ€ç»ˆç›¸ä¼¼åº¦ (ä¸ä½¿ç”¨æƒé‡)
        return max(similarities)

    def similarity_match(self, clinic_disease, standard_diseases):
        """
        ç›¸ä¼¼åº¦åŒ¹é…

        Args:
            clinic_disease (str): é—¨è¯Šç–¾ç—…å
            standard_diseases (list): æ ‡å‡†ç–¾ç—…åˆ—è¡¨

        Returns:
            list: [(ç–¾ç—…å, ç›¸ä¼¼åº¦), ...] æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        """
        similarities = []

        for standard_disease in standard_diseases:
            similarity = self.calculate_similarity(clinic_disease, standard_disease)
            similarities.append((standard_disease, similarity))

        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
        similarities.sort(key=lambda x: x[1], reverse=True)

        return similarities

    def get_cache_key(self, text):
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def load_cache(self, cache_file):
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        if not self.use_llm:
            return {}

        cache_path = os.path.join(self.cache_dir, cache_file)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_cache(self, cache_file, cache_data):
        """ä¿å­˜ç¼“å­˜æ–‡ä»¶"""
        if not self.use_llm:
            return

        cache_path = os.path.join(self.cache_dir, cache_file)
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

    def call_azure_api(self, prompt, max_retries=3):
        """
        è°ƒç”¨Azure OpenAI APIï¼ˆå¸¦ç¼“å­˜ï¼‰
        """
        if not self.use_llm:
            return None

        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self.get_cache_key(prompt)
        cache_file = "disease_matching_api_responses.json"
        cache_data = self.load_cache(cache_file)

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in cache_data:
            self.cache_hits += 1
            return cache_data[cache_key]

        # APIè°ƒç”¨
        for attempt in range(max_retries):
            try:
                self.api_calls += 1
                response = self.client.chat.completions.create(
                    model=self.deployment_name,
                    messages=[
                        {"role": "system",
                         "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ä¸­åŒ»ä¸“å®¶ï¼Œç²¾é€šä¸­åŒ»ç–¾ç—…åˆ†ç±»å’Œå‘½åè§„èŒƒï¼Œæ“…é•¿è¯†åˆ«ä¸åŒç–¾ç—…åç§°æ˜¯å¦æŒ‡å‘åŒä¸€ç§ç–¾ç—…ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                )

                if response.choices and response.choices[0].message and response.choices[0].message.content:
                    result = response.choices[0].message.content.strip()
                    # ä¿å­˜åˆ°ç¼“å­˜
                    cache_data[cache_key] = result
                    self.save_cache(cache_file, cache_data)
                    return result
                else:
                    logger.warning(f"APIè¿”å›ç©ºå“åº” (å°è¯• {attempt + 1}/{max_retries})")

            except Exception as e:
                logger.warning(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)

        logger.error("APIè°ƒç”¨æœ€ç»ˆå¤±è´¥")
        return None

    def semantic_match(self, clinic_disease, candidate_diseases):
        """
        è¯­ä¹‰åŒ¹é… (ä½¿ç”¨GPT-o3)

        Args:
            clinic_disease (str): é—¨è¯Šç–¾ç—…å
            candidate_diseases (list): å€™é€‰ç–¾ç—…åˆ—è¡¨ [(ç–¾ç—…å, ç›¸ä¼¼åº¦), ...]

        Returns:
            tuple: (æ˜¯å¦åŒ¹é…, åŒ¹é…çš„ç–¾ç—…å, ç½®ä¿¡åº¦)
        """
        if not self.use_llm or not candidate_diseases:
            return False, None, "æ— "

        # æ„å»ºå€™é€‰ç–¾ç—…åˆ—è¡¨æ–‡æœ¬
        candidates_text = []
        for i, (disease, similarity) in enumerate(candidate_diseases[:5], 1):  # æœ€å¤šå–å‰5ä¸ª
            candidates_text.append(f"{i}. {disease} (ç›¸ä¼¼åº¦: {similarity:.3f})")

        prompt = f"""
ä½œä¸ºä¸­åŒ»ä¸“å®¶ï¼Œè¯·åˆ¤æ–­é—¨è¯Šç–¾ç—…åæ˜¯å¦ä¸ä»¥ä¸‹å€™é€‰ç–¾ç—…ä¸­çš„ä»»ä½•ä¸€ä¸ªæŒ‡å‘åŒä¸€ç§ç–¾ç—…ã€‚

é—¨è¯Šç–¾ç—…åï¼š{clinic_disease}

å€™é€‰ç–¾ç—…åˆ—è¡¨ï¼š
{chr(10).join(candidates_text)}

åˆ¤æ–­æ ‡å‡†ï¼š
1. è€ƒè™‘ä¸­åŒ»ç–¾ç—…çš„å¤šç§è¡¨è¾¾æ–¹å¼å’Œå†å²æ¼”å˜
2. è€ƒè™‘å¤ä»£å’Œç°ä»£æœ¯è¯­çš„å·®å¼‚
3. è€ƒè™‘åœ°åŸŸæ€§è¡¨è¾¾å·®å¼‚
4. è€ƒè™‘æ˜¯å¦ä¸ºåŒä¸€ç–¾ç—…çš„ä¸åŒåˆ†ç±»æ–¹å¼

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "is_match": true/false,
  "matched_disease": "åŒ¹é…çš„ç–¾ç—…å" (å¦‚æœåŒ¹é…çš„è¯),
  "confidence": "é«˜/ä¸­/ä½",
  "reason": "åˆ¤æ–­ç†ç”±"
}}

å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œmatched_diseaseè®¾ä¸ºnullã€‚
"""

        response = self.call_azure_api(prompt)

        if response:
            try:
                result = json.loads(response)
                is_match = result.get('is_match', False)
                matched_disease = result.get('matched_disease')
                confidence = result.get('confidence', 'ä½')

                return is_match, matched_disease, confidence

            except json.JSONDecodeError:
                logger.warning(f"è¯­ä¹‰åŒ¹é…JSONè§£æå¤±è´¥: {clinic_disease}")
                return False, None, "ä½"

        return False, None, "ä½"

    def match_single_disease(self, clinic_disease, standard_diseases):
        """
        åŒ¹é…å•ä¸ªç–¾ç—…

        Args:
            clinic_disease (str): é—¨è¯Šç–¾ç—…å
            standard_diseases (list): æ ‡å‡†ç–¾ç—…åˆ—è¡¨

        Returns:
            dict: åŒ¹é…ç»“æœ
        """
        result = {
            'é—¨è¯Šä¸­ç–¾ç—…å': clinic_disease,
            'æ˜¯å¦åŒ¹é…': False,
            'åŒ¹é…ç±»å‹': 'æœªåŒ¹é…',
            'æœ€ä½³åŒ¹é…ç–¾ç—…': 'æ²¡æœ‰æ‰¾åˆ°åŒ¹é…ç–¾ç—…',
            'åŒ¹é…ç›¸ä¼¼åº¦': 0.0,
            'ç›¸ä¼¼åº¦åŒ¹é…ç–¾ç—…ååˆ—è¡¨': ''
        }

        # ç¬¬ä¸€å±‚ï¼šç²¾ç¡®åŒ¹é…
        is_exact, exact_match = self.exact_match(clinic_disease, standard_diseases)
        if is_exact:
            result.update({
                'æ˜¯å¦åŒ¹é…': True,
                'åŒ¹é…ç±»å‹': 'ç²¾ç¡®åŒ¹é…',
                'æœ€ä½³åŒ¹é…ç–¾ç—…': exact_match,
                'åŒ¹é…ç›¸ä¼¼åº¦': 1.0,
                'ç›¸ä¼¼åº¦åŒ¹é…ç–¾ç—…ååˆ—è¡¨': exact_match
            })
            return result

        # ç¬¬äºŒå±‚ï¼šç›¸ä¼¼åº¦åŒ¹é…
        similarities = self.similarity_match(clinic_disease, standard_diseases)

        # æ‰¾å‡ºè¶…è¿‡é˜ˆå€¼çš„å€™é€‰ç–¾ç—…
        high_similarity_candidates = [(disease, sim) for disease, sim in similarities if
                                      sim >= self.similarity_threshold]

        if high_similarity_candidates:
            best_disease, best_similarity = high_similarity_candidates[0]
            candidates_list = ','.join([disease for disease, _ in high_similarity_candidates])

            result.update({
                'æ˜¯å¦åŒ¹é…': True,
                'åŒ¹é…ç±»å‹': 'é«˜ç›¸ä¼¼åº¦åŒ¹é…',
                'æœ€ä½³åŒ¹é…ç–¾ç—…': best_disease,
                'åŒ¹é…ç›¸ä¼¼åº¦': best_similarity,
                'ç›¸ä¼¼åº¦åŒ¹é…ç–¾ç—…ååˆ—è¡¨': candidates_list
            })
            return result

        # ç¬¬ä¸‰å±‚ï¼šè¯­ä¹‰åŒ¹é… (å¯¹0.4-0.7ä¹‹é—´çš„å€™é€‰ç–¾ç—…)
        medium_similarity_candidates = [(disease, sim) for disease, sim in similarities
                                        if self.llm_threshold_low <= sim < self.llm_threshold_high]

        if medium_similarity_candidates and self.use_llm:
            is_semantic_match, matched_disease, confidence = self.semantic_match(clinic_disease,
                                                                                 medium_similarity_candidates)

            if is_semantic_match and matched_disease:
                # æ‰¾åˆ°åŒ¹é…çš„ç›¸ä¼¼åº¦
                matched_similarity = next(
                    (sim for disease, sim in medium_similarity_candidates if disease == matched_disease), 0.0)

                result.update({
                    'æ˜¯å¦åŒ¹é…': True,
                    'åŒ¹é…ç±»å‹': f'è¯­ä¹‰åŒ¹é…({confidence}ç½®ä¿¡åº¦)',
                    'æœ€ä½³åŒ¹é…ç–¾ç—…': matched_disease,
                    'åŒ¹é…ç›¸ä¼¼åº¦': matched_similarity,
                    'ç›¸ä¼¼åº¦åŒ¹é…ç–¾ç—…ååˆ—è¡¨': matched_disease
                })
                return result

        # å¦‚æœéƒ½æ²¡æœ‰åŒ¹é…ï¼Œæ˜¾ç¤ºæœ€é«˜ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if similarities:
            best_disease, best_similarity = similarities[0]
            result.update({
                'æœ€ä½³åŒ¹é…ç–¾ç—…': f'{best_disease}(ç›¸ä¼¼åº¦:{best_similarity:.3f})',
                'åŒ¹é…ç›¸ä¼¼åº¦': best_similarity
            })

        return result

    def run_matching(self, diseases_txt_path, json_path, output_path):
        """
        è¿è¡Œå®Œæ•´çš„ç–¾ç—…åŒ¹é…æµç¨‹

        Args:
            diseases_txt_path (str): é—¨è¯Šç–¾ç—…txtæ–‡ä»¶è·¯å¾„
            json_path (str): æ ‡å‡†ç–¾ç—…jsonæ–‡ä»¶è·¯å¾„
            output_path (str): è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„
        """
        start_time = time.time()

        logger.info("=" * 80)
        logger.info("å¼€å§‹ä¸­åŒ»ç–¾ç—…åŒ¹é…éªŒè¯ä»»åŠ¡")
        logger.info("=" * 80)

        try:
            # æ­¥éª¤1ï¼šåŠ è½½æ•°æ®
            logger.info("ğŸ“‹ æ­¥éª¤1: åŠ è½½æ•°æ®æ–‡ä»¶")
            clinic_diseases, standard_diseases = self.load_data(diseases_txt_path, json_path)

            # æ­¥éª¤2ï¼šæ‰¹é‡åŒ¹é…
            logger.info("ğŸ“‹ æ­¥éª¤2: å¼€å§‹æ‰¹é‡åŒ¹é…ç–¾ç—…")
            results = []

            total_diseases = len(clinic_diseases)
            for i, clinic_disease in enumerate(clinic_diseases, 1):
                logger.info(f"å¤„ç†è¿›åº¦: {i}/{total_diseases} - {clinic_disease}")

                result = self.match_single_disease(clinic_disease, standard_diseases)
                results.append(result)

                # é¿å…APIè°ƒç”¨è¿‡å¿«
                if self.use_llm and i % 10 == 0:
                    time.sleep(0.5)

            # æ­¥éª¤3ï¼šä¿å­˜ç»“æœ
            logger.info("ğŸ“‹ æ­¥éª¤3: ä¿å­˜åŒ¹é…ç»“æœ")
            df_results = pd.DataFrame(results)
            df_results.to_excel(output_path, index=False)

            # æ­¥éª¤4ï¼šç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            processing_time = end_time - start_time

            # ç»Ÿè®¡åŒ¹é…æƒ…å†µ
            total_count = len(results)
            matched_count = len([r for r in results if r['æ˜¯å¦åŒ¹é…']])
            exact_match_count = len([r for r in results if r['åŒ¹é…ç±»å‹'] == 'ç²¾ç¡®åŒ¹é…'])
            similarity_match_count = len([r for r in results if r['åŒ¹é…ç±»å‹'] == 'é«˜ç›¸ä¼¼åº¦åŒ¹é…'])
            semantic_match_count = len([r for r in results if 'è¯­ä¹‰åŒ¹é…' in r['åŒ¹é…ç±»å‹']])

            logger.info("=" * 80)
            logger.info("ğŸ“Š ç–¾ç—…åŒ¹é…å®Œæˆç»Ÿè®¡")
            logger.info("=" * 80)
            logger.info(f"å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            if self.use_llm:
                logger.info(f"APIè°ƒç”¨æ¬¡æ•°: {self.api_calls}")
                logger.info(f"ç¼“å­˜å‘½ä¸­æ¬¡æ•°: {self.cache_hits}")
                if self.api_calls + self.cache_hits > 0:
                    cache_rate = self.cache_hits / (self.api_calls + self.cache_hits) * 100
                    logger.info(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_rate:.1f}%")

            logger.info(f"æ€»ç–¾ç—…æ•°: {total_count}")
            logger.info(f"åŒ¹é…æˆåŠŸ: {matched_count} ({matched_count / total_count * 100:.1f}%)")
            logger.info(f"  - ç²¾ç¡®åŒ¹é…: {exact_match_count}")
            logger.info(f"  - é«˜ç›¸ä¼¼åº¦åŒ¹é…: {similarity_match_count}")
            if self.use_llm:
                logger.info(f"  - è¯­ä¹‰åŒ¹é…: {semantic_match_count}")
            logger.info(f"æœªåŒ¹é…: {total_count - matched_count}")
            logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
            logger.info("=" * 80)

        except Exception as e:
            logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
            raise


def main():
    """
    ä¸»å‡½æ•°
    """
    # Azure OpenAIé…ç½® (å¯é€‰ï¼Œç”¨äºè¯­ä¹‰åŒ¹é…)
    AZURE_API_KEY = ""  # å¯ä»¥ç•™ç©ºï¼Œå°†è·³è¿‡è¯­ä¹‰åŒ¹é…
    AZURE_ENDPOINT = ""  # å¯ä»¥ç•™ç©ºï¼Œå°†è·³è¿‡è¯­ä¹‰åŒ¹é…
    DEPLOYMENT_NAME = "o3"

    # æ–‡ä»¶è·¯å¾„
    diseases_txt_path = "../../data/other/dvalidated_tcm_diseases.txt"
    json_path = "../../data/other/disease_to_syndromes_merged.json"
    output_path = "../../data/result/disease_matching_results.xlsx"

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # åˆ›å»ºåŒ¹é…ç³»ç»Ÿ
    matcher = DiseaseMatchingSystem(
        azure_api_key=AZURE_API_KEY if AZURE_API_KEY else None,
        azure_endpoint=AZURE_ENDPOINT if AZURE_ENDPOINT else None,
        deployment_name=DEPLOYMENT_NAME
    )

    # è¿è¡ŒåŒ¹é…
    matcher.run_matching(diseases_txt_path, json_path, output_path)


if __name__ == "__main__":
    main()
