import pandas as pd
import json
import logging
from datetime import datetime
from collections import defaultdict
from openai import AzureOpenAI
import time
import re
import os
import hashlib
"""
æ­£å¼ç‰ˆæœ¬ï¼Œè·‘å‡ºæ¥çš„æ•°æ®ä¸ºï¼šDiseaseæè¿°åº“_ä¼˜åŒ–ç‰ˆ_20250806_121343.xlsx

ä½¿ç”¨è¯¥ç‰ˆæœ¬å…ˆå»é‡ï¼Œç„¶åå†æ•´ç†æ ¼å¼å³å¯äº¤ä»˜ã€‚
"""
# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class OptimizedMedicalDescriptionExtractor:
    """
    ä¼˜åŒ–ç‰ˆåŒ»ç–—æè¿°æå–å™¨
    - å¤§å¹…å‡å°‘APIè°ƒç”¨æ¬¡æ•°
    - å¢åŠ ä¸­é—´ç»“æœç¼“å­˜
    - æ‰¹é‡å¤„ç†æé«˜æ•ˆç‡
    """

    def __init__(self, azure_api_key, azure_endpoint, deployment_name="o3", cache_dir="cache"):
        """
        åˆå§‹åŒ–æè¿°æå–å™¨

        Args:
            azure_api_key (str): Azure OpenAI APIå¯†é’¥
            azure_endpoint (str): Azure OpenAIç«¯ç‚¹
            deployment_name (str): éƒ¨ç½²åç§°ï¼Œé»˜è®¤o3
            cache_dir (str): ç¼“å­˜ç›®å½•
        """
        # ä¿®å¤endpointæ ¼å¼
        if azure_endpoint.endswith('chat/completions?'):
            azure_endpoint = azure_endpoint.replace('/openai/deployments/o3/chat/completions?', '')

        self.client = AzureOpenAI(
            api_key=azure_api_key,
            api_version="2025-01-01-preview",
            azure_endpoint=azure_endpoint
        )
        self.deployment_name = deployment_name

        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)

        # å®šä¹‰8ä¸ªå…³é”®åŒ»ç–—å­—æ®µ
        self.key_fields = ['ä¸»è¯‰', 'ç°ç—…å²', 'æ—¢å¾€å²', 'è¾…åŠ©æ£€æŸ¥', 'PE/æ£€æŸ¥', 'ç—…æœº', 'æ²»åˆ™/å¤„ç†', 'åŒ»å˜±']

        # APIè°ƒç”¨ç»Ÿè®¡
        self.api_calls = 0
        self.cache_hits = 0

    def get_cache_key(self, text):
        """ç”Ÿæˆç¼“å­˜é”®"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def load_cache(self, cache_file):
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        cache_path = os.path.join(self.cache_dir, cache_file)
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_cache(self, cache_file, cache_data):
        """ä¿å­˜ç¼“å­˜æ–‡ä»¶"""
        cache_path = os.path.join(self.cache_dir, cache_file)
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

    def call_azure_api(self, prompt, max_retries=3):
        """
        è°ƒç”¨Azure OpenAI APIï¼ˆå¸¦ç¼“å­˜ï¼‰
        """
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self.get_cache_key(prompt)
        cache_file = "api_responses.json"
        cache_data = self.load_cache(cache_file)

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in cache_data:
            self.cache_hits += 1
            return cache_data[cache_key]

        # APIè°ƒç”¨
        for attempt in range(max_retries):
            try:
                self.api_calls += 1
                response = self.client.chat.completions.create(
                    model=self.deployment_name,
                    messages=[
                        {"role": "system",
                         "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—ä¿¡æ¯å¤„ç†ä¸“å®¶ï¼Œç²¾é€šä¸­åŒ»å’Œè¥¿åŒ»ç†è®ºï¼Œæ“…é•¿æå–å’Œæ ‡å‡†åŒ–åŒ»ç–—æè¿°ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                )

                # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
                if response.choices and response.choices[0].message and response.choices[0].message.content:
                    result = response.choices[0].message.content.strip()
                    # ä¿å­˜åˆ°ç¼“å­˜
                    cache_data[cache_key] = result
                    self.save_cache(cache_file, cache_data)
                    return result
                else:
                    logger.warning(f"APIè¿”å›ç©ºå“åº” (å°è¯• {attempt + 1}/{max_retries})")

            except Exception as e:
                logger.warning(f"APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)

        logger.error("APIè°ƒç”¨æœ€ç»ˆå¤±è´¥")
        return None

    def batch_extract_diseases(self, diagnosis_list):
        """
        æ‰¹é‡æå–Diseaseï¼ˆåªä¿ç•™ç—…åï¼Œè·³è¿‡æ‰€æœ‰è¯å‹ï¼‰

        Args:
            diagnosis_list (list): è¯Šæ–­æ–‡æœ¬åˆ—è¡¨

        Returns:
            dict: {è¯Šæ–­æ–‡æœ¬: [Diseaseåˆ—è¡¨]}
        """
        cache_file = "disease_extraction.json"
        cache_data = self.load_cache(cache_file)

        # æ£€æŸ¥å“ªäº›éœ€è¦å¤„ç†
        to_process = []
        results = {}

        for diagnosis in diagnosis_list:
            if not diagnosis or pd.isna(diagnosis):
                results[diagnosis] = []
                continue

            cache_key = self.get_cache_key(diagnosis)
            if cache_key in cache_data:
                results[diagnosis] = cache_data[cache_key]
                self.cache_hits += 1
            else:
                to_process.append(diagnosis)

        # æ‰¹é‡å¤„ç†æœªç¼“å­˜çš„è¯Šæ–­
        if to_process:
            logger.info(f"æ‰¹é‡æå–Disease: {len(to_process)} ä¸ªè¯Šæ–­")

            # æ„å»ºæ‰¹é‡å¤„ç†çš„prompt
            diagnosis_text = '\n'.join([f"{i + 1}. {diag}" for i, diag in enumerate(to_process)])

            prompt = f"""
è¯·åˆ†æä»¥ä¸‹{len(to_process)}ä¸ªä¸­åŒ»è¯Šæ–­å†…å®¹ï¼Œä¸ºæ¯ä¸ªè¯Šæ–­æå–å‡ºç‹¬ç«‹çš„Diseaseï¼š

è¯Šæ–­åˆ—è¡¨ï¼š
{diagnosis_text}

æå–è§„åˆ™ï¼š
1. **åªä¿ç•™ç—…å**ï¼šå¦‚"é¢ˆæ¤ç—…"ã€"è™šåŠ³ç—…"ã€"èƒƒèƒ€ç—…"ã€"åå¤´ç—›"ã€"è…°è‚ŒåŠ³æŸ"ç­‰
2. **å®Œå…¨è·³è¿‡è¯å‹**ï¼šä»»ä½•åŒ…å«"è¯"å­—çš„å†…å®¹éƒ½ä¸è¦æå–ï¼Œå¦‚"æ°”æ»è¡€ç˜€è¯"ã€"ç—°æ¹¿å†…è•´è¯"ç­‰
3. **æ¸…ç†æ ‡æ³¨**ï¼šå»é™¤"(å¯é€‰è¯ï¼š...)"ã€"[...]"ç­‰æ‰€æœ‰æ ‡æ³¨ä¿¡æ¯
4. **ä¸­è¥¿åŒ»ç—…åéƒ½ä¿ç•™**ï¼šä¸­åŒ»ç—…åå¦‚"é¢ˆæ¤ç—…"ã€"è™šåŠ³ç—…"ï¼Œè¥¿åŒ»ç—…åå¦‚"åå¤´ç—›"ã€"è…°è‚ŒåŠ³æŸ"

ç¤ºä¾‹ï¼š
è¾“å…¥ï¼šé¢ˆæ¤ç—…(å¯é€‰è¯ï¼šé¡¹ç—¹),æ°”æ»è¡€ç˜€è¯,è™šåŠ³ç—…,æ°”è¡€ä¸è¶³è¯
è¾“å‡ºï¼š["é¢ˆæ¤ç—…", "è™šåŠ³ç—…"]  ï¼ˆè·³è¿‡æ‰€æœ‰è¯å‹ï¼‰

è¯·è¿”å›JSONæ ¼å¼ï¼Œé”®ä¸ºè¯Šæ–­ç¼–å·(1,2,3...)ï¼Œå€¼ä¸ºå¯¹åº”çš„Diseaseåˆ—è¡¨ï¼š
{{
  "1": ["é¢ˆæ¤ç—…", "è™šåŠ³ç—…"],
  "2": ["åå¤´ç—›"],
  "3": ["è…°è‚ŒåŠ³æŸ"]
}}
"""

            response = self.call_azure_api(prompt)
            if response:
                try:
                    batch_results = json.loads(response)
                    for i, diagnosis in enumerate(to_process):
                        key = str(i + 1)
                        if key in batch_results:
                            diseases = [d.strip() for d in batch_results[key] if d.strip()]
                            results[diagnosis] = diseases
                            # ä¿å­˜åˆ°ç¼“å­˜
                            cache_key = self.get_cache_key(diagnosis)
                            cache_data[cache_key] = diseases
                        else:
                            results[diagnosis] = []

                    # ä¿å­˜ç¼“å­˜
                    self.save_cache(cache_file, cache_data)

                except json.JSONDecodeError:
                    logger.warning("æ‰¹é‡Diseaseæå–JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šé€ä¸ªå¤„ç†
                    for diagnosis in to_process:
                        results[diagnosis] = self._single_extract_disease(diagnosis)

        return results

    def _single_extract_disease(self, diagnosis_text):
        """å•ä¸ªè¯Šæ–­çš„Diseaseæå–ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼Œåªä¿ç•™ç—…åï¼Œè·³è¿‡è¯å‹ï¼‰"""
        if not diagnosis_text or pd.isna(diagnosis_text):
            return []

        diseases = []
        parts = diagnosis_text.split(',')

        for part in parts:
            part = part.strip()
            # æ¸…ç†æ ‡æ³¨
            part = re.sub(r'\(å¯é€‰è¯ï¼š[^)]+\)', '', part)
            part = re.sub(r'\[[^\]]+\]', '', part)
            part = part.strip()

            if not part:
                continue

            # è·³è¿‡æ‰€æœ‰åŒ…å«"è¯"çš„å†…å®¹
            if 'è¯' in part:
                continue

            # ä¿ç•™ç—…å
            if part:
                diseases.append(part)

        return diseases

    def batch_extract_descriptions(self, field_contents_by_field):
        """
        æ‰¹é‡æå–æè¿°ï¼ˆæŒ‰å­—æ®µåˆ†ç»„å¤„ç†ï¼‰

        Args:
            field_contents_by_field (dict): {å­—æ®µå: [å†…å®¹åˆ—è¡¨]}

        Returns:
            dict: {å­—æ®µå: {å†…å®¹: [æè¿°åˆ—è¡¨]}}
        """
        cache_file = "description_extraction.json"
        cache_data = self.load_cache(cache_file)

        results = {}

        for field_name, contents in field_contents_by_field.items():
            logger.info(f"æ‰¹é‡æå– '{field_name}' å­—æ®µæè¿°: {len(contents)} ä¸ªå†…å®¹")

            field_results = {}
            to_process = []

            # æ£€æŸ¥ç¼“å­˜
            for content in contents:
                if not content or pd.isna(content) or str(content).strip() in ['', 'æ— ', 'å¦è®¤', '-']:
                    field_results[content] = []
                    continue

                cache_key = f"{field_name}:{self.get_cache_key(str(content))}"
                if cache_key in cache_data:
                    field_results[content] = cache_data[cache_key]
                    self.cache_hits += 1
                else:
                    to_process.append(content)

            # æ‰¹é‡å¤„ç†æœªç¼“å­˜çš„å†…å®¹
            if to_process:
                # é™åˆ¶æ‰¹é‡å¤§å°ï¼Œé¿å…promptè¿‡é•¿
                batch_size = 10
                for i in range(0, len(to_process), batch_size):
                    batch = to_process[i:i + batch_size]
                    batch_results = self._batch_extract_descriptions_for_field(field_name, batch)

                    for content, descriptions in batch_results.items():
                        field_results[content] = descriptions
                        # ä¿å­˜åˆ°ç¼“å­˜
                        cache_key = f"{field_name}:{self.get_cache_key(str(content))}"
                        cache_data[cache_key] = descriptions

                    # é¿å…APIè°ƒç”¨è¿‡å¿«
                    time.sleep(0.5)

                # ä¿å­˜ç¼“å­˜
                self.save_cache(cache_file, cache_data)

            results[field_name] = field_results

        return results

    def _batch_extract_descriptions_for_field(self, field_name, contents):
        """ä¸ºå•ä¸ªå­—æ®µæ‰¹é‡æå–æè¿°"""
        contents_text = '\n'.join([f"{i + 1}. {content}" for i, content in enumerate(contents)])

        prompt = f"""
è¯·ä»ä»¥ä¸‹{len(contents)}ä¸ª{field_name}å†…å®¹ä¸­æ‰¹é‡æå–è¯­ä¹‰å®Œæ•´çš„åŒ»ç–—æè¿°ç‰‡æ®µï¼š

{field_name}å†…å®¹åˆ—è¡¨ï¼š
{contents_text}

æå–è¦æ±‚ï¼š
1. **è¯­ä¹‰å®Œæ•´**ï¼šæ¯ä¸ªæè¿°ç‰‡æ®µå¿…é¡»æ˜¯å®Œæ•´çš„ã€æœ‰æ„ä¹‰çš„åŒ»ç–—è¡¨è¿°
2. **æ—¶é—´æ ‡å‡†åŒ–**ï¼šå°†å…·ä½“æ—¶é—´æ”¹ä¸º"Nå¤©"ï¼Œå¦‚"1æœˆ"â†’"Nå¤©"ï¼Œ"æ•°å¹´"â†’"Nå¤©"
3. **ä¿ç•™åŸå§‹è¡¨è¿°**ï¼šä¸è¦æ·»åŠ é¢å¤–è¯æ±‡ï¼Œä¿æŒåŸæ–‡ç”¨è¯
4. **å»é™¤å†—ä½™**ï¼šè·³è¿‡"å¦è®¤"ã€"æ— "ã€"æ­£å¸¸"ç­‰æ— æ„ä¹‰æè¿°
5. **åŒ»ç”Ÿå¯å¤ç”¨**ï¼šæå–çš„æè¿°åº”è¯¥æ˜¯åŒ»ç”Ÿé—®è¯Šä¸­å¯ç›´æ¥ä½¿ç”¨çš„

è¯·è¿”å›JSONæ ¼å¼ï¼Œé”®ä¸ºå†…å®¹ç¼–å·(1,2,3...)ï¼Œå€¼ä¸ºå¯¹åº”çš„æè¿°åˆ—è¡¨ï¼š
{{
  "1": ["æè¿°1", "æè¿°2"],
  "2": ["æè¿°3"],
  "3": ["æè¿°4", "æè¿°5"]
}}
"""

        response = self.call_azure_api(prompt)
        results = {}

        if response:
            try:
                batch_results = json.loads(response)
                for i, content in enumerate(contents):
                    key = str(i + 1)
                    if key in batch_results and isinstance(batch_results[key], list):
                        descriptions = [d.strip() for d in batch_results[key] if d.strip()]
                        results[content] = descriptions
                    else:
                        results[content] = []
            except json.JSONDecodeError:
                logger.warning(f"æ‰¹é‡{field_name}æè¿°æå–JSONè§£æå¤±è´¥")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šè¿”å›ç©ºç»“æœ
                for content in contents:
                    results[content] = []
        else:
            # APIå¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ
            for content in contents:
                results[content] = []

        return results

    def smart_standardize_descriptions(self, all_descriptions_by_field):
        """
        æ™ºèƒ½æ ‡å‡†åŒ–æè¿°ï¼ˆå‡å°‘APIè°ƒç”¨ï¼‰
        """
        cache_file = "standardization.json"
        cache_data = self.load_cache(cache_file)

        standardized_results = {}

        for field_name, descriptions in all_descriptions_by_field.items():
            if not descriptions:
                standardized_results[field_name] = {}
                continue

            logger.info(f"æ ‡å‡†åŒ– '{field_name}' å­—æ®µ: {len(descriptions)} ä¸ªæè¿°")

            # ç”Ÿæˆå­—æ®µçº§åˆ«çš„ç¼“å­˜é”®
            desc_text = '|'.join(sorted(descriptions))
            cache_key = f"{field_name}:{self.get_cache_key(desc_text)}"

            if cache_key in cache_data:
                standardized_results[field_name] = cache_data[cache_key]
                self.cache_hits += 1
                continue

            # éœ€è¦APIå¤„ç†
            if len(descriptions) <= 1:
                # å•ä¸ªæè¿°æ— éœ€æ ‡å‡†åŒ–
                result = {desc: [desc] for desc in descriptions}
            else:
                result = self._api_standardize_descriptions(descriptions)

            standardized_results[field_name] = result
            # ä¿å­˜åˆ°ç¼“å­˜
            cache_data[cache_key] = result
            self.save_cache(cache_file, cache_data)

        return standardized_results

    def _api_standardize_descriptions(self, descriptions_list):
        """ä½¿ç”¨APIæ ‡å‡†åŒ–æè¿°"""
        descriptions_text = '\n'.join([f"{i + 1}. {desc}" for i, desc in enumerate(descriptions_list)])

        prompt = f"""
è¯·åˆ†æä»¥ä¸‹{len(descriptions_list)}ä¸ªåŒ»ç–—æè¿°ï¼Œæ‰¾å‡ºç›¸ä¼¼çš„æè¿°å¹¶è¿›è¡Œæ ‡å‡†åŒ–åˆå¹¶ï¼š

æè¿°åˆ—è¡¨ï¼š
{descriptions_text}

æ ‡å‡†åŒ–è§„åˆ™ï¼š
1. ç›¸ä¼¼ç—‡çŠ¶åˆå¹¶ï¼šå¦‚"é¢ˆè‚©ä¸é€‚Nå¤©"å’Œ"é¢ˆé¡¹ç–¼ç—›Nå¤©" â†’ "é¢ˆ[è‚©/é¡¹]ä¸é€‚Nå¤©"
2. é‡å¤æè¿°å»é‡ï¼šå®Œå…¨ç›¸åŒçš„æè¿°åªä¿ç•™ä¸€ä¸ª
3. åŒ…å«å…³ç³»å¤„ç†ï¼šå¦‚"æ°”æ»è¡€ç˜€"å’Œ"æ°”æ»è¡€ç˜€ï¼Œä¸é€šåˆ™ç—›" â†’ ä¿ç•™ç®€æ´ç‰ˆæœ¬"æ°”æ»è¡€ç˜€"

è¯·è¿”å›JSONæ ¼å¼ï¼Œé”®ä¸ºæ ‡å‡†åŒ–åçš„æè¿°ï¼Œå€¼ä¸ºå¯¹åº”çš„åŸå§‹æè¿°åˆ—è¡¨ï¼š
{{
  "æ ‡å‡†åŒ–æè¿°1": ["åŸå§‹æè¿°1", "åŸå§‹æè¿°2"],
  "æ ‡å‡†åŒ–æè¿°2": ["åŸå§‹æè¿°3"]
}}
"""

        response = self.call_azure_api(prompt)
        if response:
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                logger.warning("æ ‡å‡†åŒ–æè¿°JSONè§£æå¤±è´¥")

        # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•å»é‡
        unique_descriptions = list(set(descriptions_list))
        return {desc: [desc] for desc in unique_descriptions}

    def run(self, input_file, max_diseases=None):
        """è¿è¡Œä¼˜åŒ–çš„æè¿°æå–æµç¨‹"""
        start_time = time.time()

        logger.info("=" * 80)
        logger.info("å¼€å§‹ä¼˜åŒ–ç‰ˆåŒ»ç–—æè¿°æå–ä»»åŠ¡")
        if max_diseases:
            logger.info(f"âš ï¸  æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ {max_diseases} ä¸ªDisease")
        logger.info("=" * 80)

        try:
            # è¯»å–æ•°æ®
            logger.info(f"è¯»å–æ•°æ®æ–‡ä»¶: {input_file}")
            df = pd.read_excel(input_file)
            logger.info(f"æˆåŠŸè¯»å– {len(df)} æ¡è®°å½•")

            # ç¬¬1æ­¥ï¼šæ‰¹é‡æå–Disease
            logger.info("ğŸ“‹ æ­¥éª¤1: æ‰¹é‡æå–Disease")
            all_diagnoses = df['è¯Šæ–­'].dropna().unique().tolist()
            disease_mapping = self.batch_extract_diseases(all_diagnoses)

            # æ„å»ºDisease-è®°å½•æ˜ å°„
            disease_records = defaultdict(list)
            for index, row in df.iterrows():
                diagnosis = row.get('è¯Šæ–­', '')
                diseases = disease_mapping.get(diagnosis, [])

                for disease in diseases:
                    record_data = {
                        'æ‚£è€…ID': f"P{index + 1:03d}",
                        'åŸå§‹è¯Šæ–­': diagnosis
                    }
                    for field in self.key_fields:
                        record_data[field] = row.get(field, '') or ''
                    disease_records[disease].append(record_data)

            logger.info(f"è¯†åˆ«å‡º {len(disease_records)} ä¸ªå”¯ä¸€Disease")

            # é™åˆ¶å¤„ç†æ•°é‡
            if max_diseases:
                disease_items = list(disease_records.items())[:max_diseases]
                disease_records = dict(disease_items)
                logger.info(f"é™åˆ¶å¤„ç† {len(disease_records)} ä¸ªDisease")

            # ç¬¬2æ­¥ï¼šæ”¶é›†æ‰€æœ‰å­—æ®µå†…å®¹
            logger.info("ğŸ“‹ æ­¥éª¤2: æ”¶é›†å­—æ®µå†…å®¹")
            field_contents = defaultdict(set)

            for records in disease_records.values():
                for record in records:
                    for field in self.key_fields:
                        content = record.get(field, '')
                        if content and str(content).strip() not in ['', 'æ— ', 'å¦è®¤', '-']:
                            field_contents[field].add(str(content))

            # è½¬æ¢ä¸ºåˆ—è¡¨
            field_contents_list = {field: list(contents) for field, contents in field_contents.items()}

            # ç¬¬3æ­¥ï¼šæ‰¹é‡æå–æè¿°
            logger.info("ğŸ“‹ æ­¥éª¤3: æ‰¹é‡æå–æè¿°")
            description_mapping = self.batch_extract_descriptions(field_contents_list)

            # ç¬¬4æ­¥ï¼šæ”¶é›†æ‰€æœ‰æè¿°å¹¶æ ‡å‡†åŒ–
            logger.info("ğŸ“‹ æ­¥éª¤4: æ ‡å‡†åŒ–æè¿°")
            all_descriptions_by_field = defaultdict(list)

            for field_name, content_desc_map in description_mapping.items():
                for descriptions in content_desc_map.values():
                    all_descriptions_by_field[field_name].extend(descriptions)

            # å»é‡
            for field in all_descriptions_by_field:
                all_descriptions_by_field[field] = list(set(all_descriptions_by_field[field]))

            # æ‰¹é‡æ ‡å‡†åŒ–
            standardized_mapping = self.smart_standardize_descriptions(all_descriptions_by_field)

            # ç¬¬5æ­¥ï¼šç”Ÿæˆæœ€ç»ˆç»“æœ
            logger.info("ğŸ“‹ æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆç»“æœ")
            final_results = []

            for disease, records in disease_records.items():
                logger.info(f"å¤„ç†Disease: {disease} ({len(records)} æ¡è®°å½•)")

                # ç»Ÿè®¡è¯¥Diseaseçš„æ‰€æœ‰æè¿°
                disease_descriptions = defaultdict(lambda: defaultdict(list))

                for record in records:
                    for field in self.key_fields:
                        content = record.get(field, '')
                        if content and field in description_mapping:
                            descriptions = description_mapping[field].get(content, [])
                            for desc in descriptions:
                                # æ‰¾åˆ°æ ‡å‡†åŒ–åçš„æè¿°
                                standard_desc = desc
                                if field in standardized_mapping:
                                    for std_desc, orig_list in standardized_mapping[field].items():
                                        if desc in orig_list:
                                            standard_desc = std_desc
                                            break

                                disease_descriptions[field][standard_desc].append(record['æ‚£è€…ID'])

                # ç”Ÿæˆç»“æœ
                for field, desc_patients in disease_descriptions.items():
                    for desc, patient_ids in desc_patients.items():
                        unique_patients = list(set(patient_ids))
                        final_results.append({
                            'Disease': disease,
                            'æè¿°å†…å®¹': desc,
                            'æ¥æºå­—æ®µ': field,
                            'å‡ºç°æ¬¡æ•°': len(patient_ids),
                            'æ ·æœ¬æ‚£è€…æ•°': len(unique_patients),
                            'æ ·æœ¬æ‚£è€…ID': ','.join(unique_patients[:10])
                        })

            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            if max_diseases:
                output_file = f"Diseaseæè¿°åº“_ä¼˜åŒ–æµ‹è¯•{max_diseases}ä¸ª_{timestamp}.xlsx"
            else:
                output_file = f"Diseaseæè¿°åº“_ä¼˜åŒ–ç‰ˆ_{timestamp}.xlsx"

            df_results = pd.DataFrame(final_results)
            df_results.to_excel(output_file, index=False)

            # ç»Ÿè®¡ä¿¡æ¯
            end_time = time.time()
            processing_time = end_time - start_time

            logger.info("=" * 80)
            logger.info("ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡")
            logger.info("=" * 80)
            logger.info(f"å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            logger.info(f"APIè°ƒç”¨æ¬¡æ•°: {self.api_calls}")
            logger.info(f"ç¼“å­˜å‘½ä¸­æ¬¡æ•°: {self.cache_hits}")
            logger.info(f"ç¼“å­˜å‘½ä¸­ç‡: {self.cache_hits / (self.api_calls + self.cache_hits) * 100:.1f}%")
            logger.info(f"å¤„ç†Diseaseæ•°é‡: {len(disease_records)}")
            logger.info(f"ç”Ÿæˆæè¿°æ€»æ•°: {len(final_results)}")
            logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
            logger.info("=" * 80)

        except Exception as e:
            logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    # Azure OpenAIé…ç½®
    AZURE_API_KEY = ""
    AZURE_ENDPOINT = ""  # ä¿®æ­£åçš„ç«¯ç‚¹
    DEPLOYMENT_NAME = "o3"

    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    input_file = "../data/case_data/ç—…å†æ•°æ®_å¯ä½¿ç”¨_20250804_172720.xlsx"

    # æµ‹è¯•é…ç½®
    MAX_DISEASES = None  # æ”¹ä¸ºNoneå¯å¤„ç†å…¨éƒ¨Disease

    # åˆ›å»ºæå–å™¨å¹¶è¿è¡Œ
    extractor = OptimizedMedicalDescriptionExtractor(
        azure_api_key=AZURE_API_KEY,
        azure_endpoint=AZURE_ENDPOINT,
        deployment_name=DEPLOYMENT_NAME
    )

    # è¿è¡Œå¤„ç†æµç¨‹
    extractor.run(input_file, max_diseases=MAX_DISEASES)


if __name__ == "__main__":
    main()
